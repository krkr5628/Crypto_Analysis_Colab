{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgeLUMvxkER9OLKPSMLpP+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU0xC3VcLHTT"
      },
      "outputs": [],
      "source": [
        "pip install pandas_ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ta"
      ],
      "metadata": {
        "id": "_x6FdHv2mEx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "from tqdm import tqdm\n",
        "import ta\n",
        "import pandas_ta as pata"
      ],
      "metadata": {
        "id": "gMog1bgamF4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i_zZSS6CmS-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**데이터 호출**\n",
        "######################"
      ],
      "metadata": {
        "id": "xkS1dSyMmZbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path = '/content/drive/MyDrive/Data/SOL_Data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "5HN2VdF6mW0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path = '/content/drive/MyDrive/Data/SOL60_INDICATOR3'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Rp89hBoSoOKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path = '/content/drive/MyDrive/Data/SOL_Data_30m.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "5bFzfFnnoQ5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path = '/content/drive/MyDrive/Data/SOL_Data_1m_Micro_Indicator3.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "6ZDZDQyQoSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임의 길이 계산\n",
        "total_length = len(data)\n",
        "\n",
        "# 4등분으로 나눈 길이 계산\n",
        "split_length = total_length // 4\n",
        "\n",
        "# 두번째 부분 데이터 분리\n",
        "data = data.iloc[split_length*2:split_length*3]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "uskClwdknehu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVE\n",
        "file_path2 = '/content/drive/MyDrive/Data/SOL_Data_Micro.csv'\n",
        "data.to_csv(file_path2, index=False);"
      ],
      "metadata": {
        "id": "ibh1Llwkncvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**테스트 데이터 분리**\n",
        "######################"
      ],
      "metadata": {
        "id": "9ciI-4H1mm99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임의 길이 계산\n",
        "total_length = len(data)\n",
        "\n",
        "# 4등분으로 나눈 길이 계산\n",
        "split_length = total_length // 4\n",
        "\n",
        "# 두번째 부분 데이터 분리\n",
        "data = data.iloc[split_length*2:split_length*3]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "8ZyCogfdmr_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVE\n",
        "file_path2 = '/content/drive/MyDrive/Data/SOL_Data_Test.csv'\n",
        "data.to_csv(path2, index=False);"
      ],
      "metadata": {
        "id": "iErIqFTenKKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**데이터 지표 추가**\n",
        "######################"
      ],
      "metadata": {
        "id": "vTT5CwKgnE9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open_time 열을 datetime 형식으로 변환\n",
        "data['open_time'] = pd.to_datetime(data['open_time'])\n",
        "\n",
        "# 시각과 분만 추출하여 time 열 생성\n",
        "data['time'] = data['open_time'].dt.strftime('%H:%M')\n",
        "\n",
        "# ATR 계산 함수\n",
        "def calculate_atr(df, periods):\n",
        "    for period in periods:\n",
        "        df[f'atr_{period}'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'], window=period)\n",
        "    return df\n",
        "\n",
        "# VWAP 계산 함수\n",
        "def calculate_vwap(df):\n",
        "    df['vwap'] = ta.volume.volume_weighted_average_price(df['high'], df['low'], df['close'], df['volume'])\n",
        "    return df\n",
        "\n",
        "# Stochastic Oscillator 계산 함수\n",
        "def calculate_stoch(df, periods):\n",
        "    for period, smooth in periods:\n",
        "        df[f'stoch_%k_{period}_{smooth}'] = ta.momentum.stoch(df['high'], df['low'], df['close'], window=period, smooth_window=smooth)\n",
        "        df[f'stoch_%d_{period}_{smooth}'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'], window=period, smooth_window=smooth)\n",
        "    return df\n",
        "\n",
        "# OBV 계산 함수\n",
        "def calculate_obv(df):\n",
        "    df['obv'] = ta.volume.on_balance_volume(df['close'], df['volume'])\n",
        "    return df\n",
        "\n",
        "# Bollinger Bands 계산 함수\n",
        "def calculate_bollinger_bands(df, periods):\n",
        "    for period in periods:\n",
        "        bollinger = ta.volatility.BollingerBands(df['close'], window=period)\n",
        "        df[f'bollinger_hband_{period}'] = bollinger.bollinger_hband()\n",
        "        df[f'bollinger_lband_{period}'] = bollinger.bollinger_lband()\n",
        "    return df\n",
        "\n",
        "# Ichimoku 계산 함수\n",
        "def calculate_ichimoku(df, periods):\n",
        "    for period in periods:\n",
        "        df[f'ichimoku_base_{period}'] = ta.trend.ichimoku_base_line(df['high'], df['low'], window1=period)\n",
        "        df[f'ichimoku_conversion_{period}'] = ta.trend.ichimoku_conversion_line(df['high'], df['low'], window1=period)\n",
        "    return df\n",
        "\n",
        "# Supertrend 계산 함수\n",
        "def calculate_supertrend(df, settings):\n",
        "    df = df.copy()\n",
        "    for period, multiplier, atr_period in settings:\n",
        "        hl2 = (df['high'] + df['low']) / 2\n",
        "        df['atr'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'], window=atr_period)\n",
        "        df['upperband'] = hl2 + (multiplier * df['atr'])\n",
        "        df['lowerband'] = hl2 - (multiplier * df['atr'])\n",
        "        df['in_uptrend'] = True\n",
        "\n",
        "        for current in tqdm(range(1, len(df.index)), desc=f'Calculating Supertrend {period}-{multiplier}-{atr_period}', leave=False):\n",
        "            previous = current - 1\n",
        "\n",
        "            if df['close'].iloc[current] > df['upperband'].iloc[previous]:\n",
        "                df.loc[df.index[current], 'in_uptrend'] = True\n",
        "            elif df['close'].iloc[current] < df['lowerband'].iloc[previous]:\n",
        "                df.loc[df.index[current], 'in_uptrend'] = False\n",
        "            else:\n",
        "                df.loc[df.index[current], 'in_uptrend'] = df['in_uptrend'].iloc[previous]\n",
        "\n",
        "                if df['in_uptrend'].iloc[current] and df['lowerband'].iloc[current] < df['lowerband'].iloc[previous]:\n",
        "                    df.loc[df.index[current], 'lowerband'] = df['lowerband'].iloc[previous]\n",
        "\n",
        "                if not df['in_uptrend'].iloc[current] and df['upperband'].iloc[current] > df['upperband'].iloc[previous]:\n",
        "                    df.loc[df.index[current], 'upperband'] = df['upperband'].iloc[previous]\n",
        "\n",
        "        df[f'supertrend_upper_{period}_{multiplier}_{atr_period}'] = df['upperband']\n",
        "        df[f'supertrend_lower_{period}_{multiplier}_{atr_period}'] = df['lowerband']\n",
        "        df[f'supertrend_in_uptrend_{period}_{multiplier}_{atr_period}'] = df['in_uptrend']\n",
        "    return df\n",
        "\n",
        "# 모든 계산 수행\n",
        "atr_periods = [5, 10, 14, 20, 50]\n",
        "stoch_periods = [(14, 3), (21, 5), (9, 3), (5, 2), (20, 7)]\n",
        "bollinger_periods = [10, 20, 50, 100, 200]\n",
        "ichimoku_periods = [9, 26, 52, 100, 200]\n",
        "supertrend_settings = [(7, 3, 14), (10, 3, 20), (14, 2, 10), (20, 4, 50), (50, 5, 5)]\n",
        "\n",
        "data = calculate_atr(data, atr_periods)\n",
        "data = calculate_vwap(data)\n",
        "data = calculate_stoch(data, stoch_periods)\n",
        "data = calculate_obv(data)\n",
        "data = calculate_bollinger_bands(data, bollinger_periods)\n",
        "data = calculate_ichimoku(data, ichimoku_periods)\n",
        "data = calculate_supertrend(data, supertrend_settings)"
      ],
      "metadata": {
        "id": "a0tyZAeKoa0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 지표 계산 함수 정의\n",
        "def calculate_indicators(df):\n",
        "    # Parabolic SAR\n",
        "    df['Parabolic_SAR_0.02'] = pata.psar(df['high'], df['low'], df['close'], af=0.02, max_af=0.2)['PSARl_0.02_0.2']\n",
        "    df['Parabolic_SAR_0.04'] = pata.psar(df['high'], df['low'], df['close'], af=0.04, max_af=0.2)['PSARl_0.04_0.2']\n",
        "    df['Parabolic_SAR_0.06'] = pata.psar(df['high'], df['low'], df['close'], af=0.06, max_af=0.2)['PSARl_0.06_0.2']\n",
        "    df['Parabolic_SAR_0.08'] = pata.psar(df['high'], df['low'], df['close'], af=0.08, max_af=0.2)['PSARl_0.08_0.2']\n",
        "    df['Parabolic_SAR_0.1'] = pata.psar(df['high'], df['low'], df['close'], af=0.1, max_af=0.2)['PSARl_0.1_0.2']\n",
        "\n",
        "    # Williams %R\n",
        "    df['Williams_%R_10'] = pata.willr(df['high'], df['low'], df['close'], length=10)\n",
        "    df['Williams_%R_20'] = pata.willr(df['high'], df['low'], df['close'], length=20)\n",
        "    df['Williams_%R_30'] = pata.willr(df['high'], df['low'], df['close'], length=30)\n",
        "    df['Williams_%R_40'] = pata.willr(df['high'], df['low'], df['close'], length=40)\n",
        "    df['Williams_%R_50'] = pata.willr(df['high'], df['low'], df['close'], length=50)\n",
        "\n",
        "    # Momentum\n",
        "    df['Momentum_10'] = pata.mom(df['close'], length=10)\n",
        "    df['Momentum_20'] = pata.mom(df['close'], length=20)\n",
        "    df['Momentum_30'] = pata.mom(df['close'], length=30)\n",
        "    df['Momentum_40'] = pata.mom(df['close'], length=40)\n",
        "    df['Momentum_50'] = pata.mom(df['close'], length=50)\n",
        "\n",
        "    # Rate of Change (ROC)\n",
        "    df['ROC_10'] = pata.roc(df['close'], length=10)\n",
        "    df['ROC_20'] = pata.roc(df['close'], length=20)\n",
        "    df['ROC_30'] = pata.roc(df['close'], length=30)\n",
        "    df['ROC_40'] = pata.roc(df['close'], length=40)\n",
        "    df['ROC_50'] = pata.roc(df['close'], length=50)\n",
        "\n",
        "    # Chande Momentum Oscillator (CMO)\n",
        "    df['CMO_10'] = pata.cmo(df['close'], length=10)\n",
        "    df['CMO_20'] = pata.cmo(df['close'], length=20)\n",
        "    df['CMO_30'] = pata.cmo(df['close'], length=30)\n",
        "    df['CMO_40'] = pata.cmo(df['close'], length=40)\n",
        "    df['CMO_50'] = pata.cmo(df['close'], length=50)\n",
        "\n",
        "    # Money Flow Index (MFI)\n",
        "    df['MFI_10'] = pata.mfi(df['high'], df['low'], df['close'], df['volume'], length=10)\n",
        "    df['MFI_20'] = pata.mfi(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
        "    df['MFI_30'] = pata.mfi(df['high'], df['low'], df['close'], df['volume'], length=30)\n",
        "    df['MFI_40'] = pata.mfi(df['high'], df['low'], df['close'], df['volume'], length=40)\n",
        "    df['MFI_50'] = pata.mfi(df['high'], df['low'], df['close'], df['volume'], length=50)\n",
        "\n",
        "    # Relative Strength Index (RSI)\n",
        "    df['RSI_10'] = pata.rsi(df['close'], length=10)\n",
        "    df['RSI_20'] = pata.rsi(df['close'], length=20)\n",
        "    df['RSI_30'] = pata.rsi(df['close'], length=30)\n",
        "    df['RSI_40'] = pata.rsi(df['close'], length=40)\n",
        "    df['RSI_50'] = pata.rsi(df['close'], length=50)\n",
        "\n",
        "    # Accumulation/Distribution Line (A/D Line)\n",
        "    df['Accumulation_Distribution_Line'] = pata.ad(df['high'], df['low'], df['close'], df['volume'])\n",
        "\n",
        "    # Elder's Force Index (EFI)\n",
        "    df['Elder_Force_Index_2'] = pata.efi(df['close'], df['volume'], length=2)\n",
        "    df['Elder_Force_Index_13'] = pata.efi(df['close'], df['volume'], length=13)\n",
        "    df['Elder_Force_Index_5'] = pata.efi(df['close'], df['volume'], length=5)\n",
        "    df['Elder_Force_Index_10'] = pata.efi(df['close'], df['volume'], length=10)\n",
        "    df['Elder_Force_Index_25'] = pata.efi(df['close'], df['volume'], length=25)\n",
        "\n",
        "    # Relative Vigor Index (RVI)\n",
        "    df['Relative_Vigor_Index_10'] = pata.rsi(df['close'], length=10)  # pandas-ta는 RVI가 없으므로 RSI로 대체\n",
        "    df['Relative_Vigor_Index_20'] = pata.rsi(df['close'], length=20)\n",
        "    df['Relative_Vigor_Index_30'] = pata.rsi(df['close'], length=30)\n",
        "    df['Relative_Vigor_Index_40'] = pata.rsi(df['close'], length=40)\n",
        "    df['Relative_Vigor_Index_50'] = pata.rsi(df['close'], length=50)\n",
        "\n",
        "    # Volume Ratio (VR)\n",
        "    def volume_ratio(close, volume, period):\n",
        "        vr = []\n",
        "        for i in range(len(close)):\n",
        "            if i < period:\n",
        "                vr.append(None)\n",
        "            else:\n",
        "                vol_up = sum(volume[j] for j in range(i - period + 1, i + 1) if close[j] > close[j - 1])\n",
        "                vol_down = sum(volume[j] for j in range(i - period + 1, i + 1) if close[j] < close[j - 1])\n",
        "                vol_same = sum(volume[j] for j in range(i - period + 1, i + 1) if close[j] == close[j - 1])\n",
        "                vr_value = (vol_up + vol_same / 2) / (vol_down + vol_same / 2) * 100\n",
        "                vr.append(vr_value)\n",
        "        return vr\n",
        "\n",
        "    df['VR_10'] = volume_ratio(df['close'], df['volume'], period=10)\n",
        "    df['VR_20'] = volume_ratio(df['close'], df['volume'], period=20)\n",
        "    df['VR_30'] = volume_ratio(df['close'], df['volume'], period=30)\n",
        "    df['VR_40'] = volume_ratio(df['close'], df['volume'], period=40)\n",
        "    df['VR_50'] = volume_ratio(df['close'], df['volume'], period=50)\n",
        "\n",
        "    # Commodity Channel Index (CCI)\n",
        "    df['CCI_10'] = pata.cci(df['high'], df['low'], df['close'], length=10)\n",
        "    df['CCI_20'] = pata.cci(df['high'], df['low'], df['close'], length=20)\n",
        "    df['CCI_30'] = pata.cci(df['high'], df['low'], df['close'], length=30)\n",
        "    df['CCI_40'] = pata.cci(df['high'], df['low'], df['close'], length=40)\n",
        "    df['CCI_50'] = pata.cci(df['high'], df['low'], df['close'], length=50)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 지표 계산\n",
        "data = calculate_indicators(data)"
      ],
      "metadata": {
        "id": "Ky5wDdPqobj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이격도(Price Disparity Index) 계산 함수\n",
        "def calculate_disparity_index(df, period):\n",
        "    df[f'disparity_index_{period}'] = (df['close'] / df['close'].rolling(window=period).mean()) * 100\n",
        "    return df\n",
        "\n",
        "# 이평선(이동평균선) 계산 함수\n",
        "def calculate_moving_averages(df, periods):\n",
        "    for period in periods:\n",
        "        df[f'price_ma_{period}'] = df['close'].rolling(window=period).mean()\n",
        "        df[f'volume_ma_{period}'] = df['volume'].rolling(window=period).mean()\n",
        "    return df\n",
        "\n",
        "# 이격도와 이동평균선 계산\n",
        "disparity_periods = [5, 10, 20, 50, 100, 200]\n",
        "moving_average_periods = [5, 10, 20, 50, 100, 200]\n",
        "\n",
        "# 이격도 계산\n",
        "for period in disparity_periods:\n",
        "    data = calculate_disparity_index(data, period)\n",
        "\n",
        "# 이동평균선 계산\n",
        "data = calculate_moving_averages(data, moving_average_periods)"
      ],
      "metadata": {
        "id": "j-mckYKhoe8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_max_min_returns(df):\n",
        "    window_size = 60\n",
        "\n",
        "    # 'open_time' 열이 데이터프레임에 있는지 확인\n",
        "    if 'open_time' not in df.columns:\n",
        "        raise KeyError(\"'open_time' 열이 데이터프레임에 포함되어 있어야 합니다.\")\n",
        "\n",
        "    # 'open_time' 열을 datetime으로 변환\n",
        "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
        "\n",
        "    # 인덱스 중복 확인 및 제거\n",
        "    df = df.drop_duplicates(subset='open_time', keep='first').copy()\n",
        "\n",
        "    # 인덱스를 'open_time'으로 설정\n",
        "    df.set_index('open_time', inplace=True)\n",
        "\n",
        "    # 현재 가격\n",
        "    current_price = df['close']\n",
        "\n",
        "    # 60분 윈도우를 적용하여 최대 및 최소 가격 계산\n",
        "    rolling_max = df['high'].rolling(window=window_size, min_periods=1).max().shift(-window_size)\n",
        "    rolling_min = df['low'].rolling(window=window_size, min_periods=1).min().shift(-window_size)\n",
        "\n",
        "    # 현재 행의 close와 60분 윈도우 내의 최대 high와 최소 low를 비교하여 수익률 계산\n",
        "    df['max_return_60min'] = ((rolling_max - current_price) / current_price) * 100\n",
        "    df['min_return_60min'] = ((rolling_min - current_price) / current_price) * 100\n",
        "\n",
        "    # 결측값을 적절히 처리 (예: 마지막 몇 행)\n",
        "    df['max_return_60min'].fillna(0, inplace=True)\n",
        "    df['min_return_60min'].fillna(0, inplace=True)\n",
        "\n",
        "    # 인덱스 리셋\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 최대 상승률과 최대 하락률 계산\n",
        "data = calculate_max_min_returns(data)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Rer5ILbEohpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "cuqbKjlfojmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.columns)"
      ],
      "metadata": {
        "id": "jau89mv1olyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path3 = \"/content/drive/MyDrive/Data/SOL_Data_30m_Indicator3.csv\";\n",
        "data.to_csv(file_path3, index=False);"
      ],
      "metadata": {
        "id": "1pm1aGEUom9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**데이터 지표 중요도 파악(데이터 처리)**\n",
        "######################"
      ],
      "metadata": {
        "id": "deC7rUEzsSjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시간 관련 열 변환 함수\n",
        "def convert_time_features(data):\n",
        "    # open_time 열이 datetime 형식이 아닌 경우 변환\n",
        "    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n",
        "        data['open_time'] = pd.to_datetime(data['open_time'])\n",
        "\n",
        "    # time 열을 분 단위로 변환\n",
        "    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n",
        "\n",
        "    # 사용하지 않을 열 제외\n",
        "    #data = data.drop(columns=['open_time', 'Unnamed: 0'])\n",
        "    data = data.drop(columns=['open_time'])\n",
        "\n",
        "    return data\n",
        "\n",
        "# 시간 관련 열 변환\n",
        "data = convert_time_features(data)"
      ],
      "metadata": {
        "id": "XpV5wuUpsUXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "oNx7MLH4ucOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data = data.drop(columns=['Unnamed: 0.1'])"
      ],
      "metadata": {
        "id": "u6I_zmasudmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data = data.drop(columns=['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "Q6Kbmyhxula2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수\n",
        "def preprocess_data(data):\n",
        "    # 목표 변수 생성\n",
        "    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n",
        "\n",
        "    # 특성과 목표 변수 분리\n",
        "    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n",
        "    y = data['target']\n",
        "\n",
        "    # 무한대 값을 NaN으로 대체\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # NaN 값을 평균으로 대체\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# 시계열 데이터 형태로 변환 함수\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        label = target[i + sequence_length - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 데이터 전처리\n",
        "X_scaled, y = preprocess_data(data)\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "# 데이터 길이 체크\n",
        "if len(X_scaled) < sequence_length:\n",
        "    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n",
        "\n",
        "# 시퀀스 데이터 생성\n",
        "y_array = y.values  # pandas Series를 numpy array로 변환\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "# 생성된 시퀀스 데이터의 형태 확인\n",
        "print(f\"X_seq shape: {X_seq.shape}\")\n",
        "print(f\"y_seq shape: {y_seq.shape}\")"
      ],
      "metadata": {
        "id": "OapDCPCcuuIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수 V2\n",
        "def preprocess_data(data):\n",
        "    # 목표 변수 생성\n",
        "    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n",
        "\n",
        "    # 피처 열만 선택\n",
        "    feature_columns = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10',\n",
        "                       'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30',\n",
        "                       'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200',\n",
        "                       'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20',\n",
        "                       'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200',\n",
        "                       'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20',\n",
        "                       'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40',\n",
        "                       'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20',\n",
        "                       'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close',\n",
        "                       'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50',\n",
        "                       'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n",
        "    # 특성과 목표 변수 분리\n",
        "    X = data[feature_columns]\n",
        "    y = data['target']\n",
        "\n",
        "    # 무한대 값을 NaN으로 대체\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # NaN 값을 평균으로 대체\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# 시계열 데이터 형태로 변환 함수\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        label = target[i + sequence_length - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 데이터 전처리\n",
        "X_scaled, y = preprocess_data(data)\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "# 데이터 길이 체크\n",
        "if len(X_scaled) < sequence_length:\n",
        "    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n",
        "\n",
        "# 시퀀스 데이터 생성\n",
        "y_array = y.values  # pandas Series를 numpy array로 변환\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "# 생성된 시퀀스 데이터의 형태 확인\n",
        "print(f\"X_seq shape: {X_seq.shape}\")\n",
        "print(f\"y_seq shape: {y_seq.shape}\")"
      ],
      "metadata": {
        "id": "eh93JJLEuvA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**데이터 지표 중요도 파악(XGBClassifier)**\n",
        "######################"
      ],
      "metadata": {
        "id": "RN7DottKqSrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "HPXrVLViqU97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "Wiltsi03sASs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost 모델 학습\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 피처 중요도 평가\n",
        "feature_importances = model.feature_importances_\n",
        "feature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns"
      ],
      "metadata": {
        "id": "ND1wVHV-sBOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처 중요도를 데이터프레임으로 변환\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# 중요도에 따라 정렬\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 피처 중요도 표 출력\n",
        "feature_importance_df.head()"
      ],
      "metadata": {
        "id": "9P8NuCPgsIyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "path1 = '/content/drive/MyDrive/Data/feature_importance_xgboost.csv'\n",
        "feature_importance_df.to_csv(path1, index=False);"
      ],
      "metadata": {
        "id": "0xiMC8pFsLVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**데이터 지표 중요도 파악(LogisticRegression)**\n",
        "######################"
      ],
      "metadata": {
        "id": "ucYG3708r8XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SequentialFeatureSelector"
      ],
      "metadata": {
        "id": "6lF2vw2mshuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression 모델 설정\n",
        "model = LogisticRegression(max_iter=5000, tol=1e-3, n_jobs=-1)  # max_iter 축소 및 tol 설정\n",
        "\n",
        "# 순차적 전진 선택\n",
        "sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction='forward', cv=3, n_jobs=-1)  # n_features_to_select 명시 및 cv 축소\n",
        "sfs.fit(X_train, y_train)\n",
        "\n",
        "# 선택된 피처\n",
        "selected_features = sfs.get_support(indices=True)\n",
        "selected_feature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns[selected_features]\n",
        "\n",
        "# 선택된 피처 중요도 계산\n",
        "model.fit(sfs.transform(X_train), y_train)\n",
        "feature_importances = model.coef_[0]\n",
        "\n",
        "# 피처 중요도를 데이터프레임으로 변환 및 정렬\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': selected_feature_names,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 피처 중요도 표 출력\n",
        "feature_importance_df.head()"
      ],
      "metadata": {
        "id": "9UV3Z1kpsi1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "path1 = '/content/drive/MyDrive/Data/feature_importance_LogisticRegression.csv'\n",
        "feature_importance_df.to_csv(path1, index=False);"
      ],
      "metadata": {
        "id": "hCFaqcIpsqNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}