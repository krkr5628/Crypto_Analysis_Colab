{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9122986,"sourceType":"datasetVersion","datasetId":5507351},{"sourceId":90787,"sourceType":"modelInstanceVersion","modelInstanceId":69639,"modelId":94706}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install imbalanced-learn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tdqm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport logging\n\n# TensorFlow 및 TPU 설정\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.impute import SimpleImputer\nimport joblib\nimport matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:16:13.733968Z","iopub.execute_input":"2024-08-07T13:16:13.734342Z","iopub.status.idle":"2024-08-07T13:16:19.045679Z","shell.execute_reply.started":"2024-08-07T13:16:13.734301Z","shell.execute_reply":"2024-08-07T13:16:19.044785Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1723036574.915628     237 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0807 13:16:14.924402535     237 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0807 13:16:14.924420163     237 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0807 13:16:14.924424059     237 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0807 13:16:14.924426827     237 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0807 13:16:14.924429655     237 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0807 13:16:14.924432358     237 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0807 13:16:14.924435048     237 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0807 13:16:14.924437680     237 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0807 13:16:14.924440261     237 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0807 13:16:14.924442809     237 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0807 13:16:14.924445411     237 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0807 13:16:14.924448032     237 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0807 13:16:14.924450643     237 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0807 13:16:14.924453224     237 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0807 13:16:14.924455809     237 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0807 13:16:14.924458384     237 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0807 13:16:14.924461133     237 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0807 13:16:14.924463761     237 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0807 13:16:14.924466378     237 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0807 13:16:14.924469014     237 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0807 13:16:14.924471615     237 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0807 13:16:14.924474206     237 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0807 13:16:14.924476932     237 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0807 13:16:14.924479593     237 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0807 13:16:14.924482138     237 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0807 13:16:14.924484692     237 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0807 13:16:14.924487380     237 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0807 13:16:14.924489984     237 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0807 13:16:14.924492700     237 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0807 13:16:14.924496568     237 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0807 13:16:14.924499266     237 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0807 13:16:14.924501937     237 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0807 13:16:14.924504636     237 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0807 13:16:14.924507233     237 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0807 13:16:14.924509828     237 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0807 13:16:14.924512436     237 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0807 13:16:14.924515029     237 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0807 13:16:14.924517640     237 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0807 13:16:14.924520291     237 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0807 13:16:14.924522942     237 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0807 13:16:14.924525520     237 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0807 13:16:14.924528074     237 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0807 13:16:14.924530738     237 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0807 13:16:14.924533394     237 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0807 13:16:14.924567702     237 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0807 13:16:14.924797546     237 ev_epoll1_linux.cc:123]               grpc epoll fd: 58\nD0807 13:16:14.924813870     237 ev_posix.cc:113]                      Using polling engine: epoll1\nD0807 13:16:14.935028100     237 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0807 13:16:14.935043512     237 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0807 13:16:14.935056200     237 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0807 13:16:14.935062077     237 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0807 13:16:14.935067461     237 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0807 13:16:14.935072684     237 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0807 13:16:14.935124386     237 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0807 13:16:14.935152274     237 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0807 13:16:14.935186189     237 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0807 13:16:14.935237572     237 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0807 13:16:14.935249763     237 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0807 13:16:14.935255321     237 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0807 13:16:14.935262090     237 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0807 13:16:14.935267460     237 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0807 13:16:14.935274823     237 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0807 13:16:14.935280403     237 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0807 13:16:14.935331646     237 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0807 13:16:14.937141011     237 ev_epoll1_linux.cc:359]               grpc epoll fd: 60\nI0807 13:16:14.938534045     237 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0807 13:16:14.942182209     332 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0807 13:16:14.942240744     332 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0807 13:16:14.947912950     237 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-08-07T13:16:14.947891746+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**학습 데이터 처리**","metadata":{}},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터2\ndata = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL60_INDICATOR3_180\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 두번째\n# 데이터프레임의 길이 계산\ntotal_length = len(data)\n\n# 3등분으로 나눈 길이 계산\nsplit_length = total_length // 3\n\n# 중간 부분 데이터 분리\ndata = data.iloc[split_length:split_length*2]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 마지막 등분\n# 데이터프레임의 길이 계산\ntotal_length = len(data)\n\n# 마지막 1/3 데이터 분리\ndata = data.iloc[-(total_length // 3):]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows_2_3 = int(len(data) * 2 / 3)\n\n# Extract the first 2/3 of the dataset\ndata = data.iloc[:rows_2_3]\n\n# Display the first few rows of the extracted dataset to ensure it is correct\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-test/SOL60_INDICATOR3_SMALL\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata = data.drop(columns=['Unnamed: 0.1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n\n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n\n    # 사용하지 않을 열 제외\n    #data = data.drop(columns=['open_time', 'Unnamed: 0'])\n    data = data.drop(columns=['open_time'])\n    \n    return data\n\n# 시간 관련 열 변환\ndata = convert_time_features(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 전처리 함수\ndef preprocess_data(data):\n    # 목표 변수 생성\n    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n\n    # 특성과 목표 변수 분리\n    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n    y = data['target']\n\n    # 무한대 값을 NaN으로 대체\n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    X_imputed = imputer.fit_transform(X)\n\n    # 데이터 정규화\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X_imputed)\n\n    return X_scaled, y\n\n# 시계열 데이터 형태로 변환 함수\ndef create_sequences(data, target, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        label = target[i + sequence_length - 1]\n        sequences.append(seq)\n        targets.append(label)\n    return np.array(sequences), np.array(targets)\n\n# 데이터 전처리\nX_scaled, y = preprocess_data(data)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 데이터 길이 체크\nif len(X_scaled) < sequence_length:\n    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n\n# 시퀀스 데이터 생성\ny_array = y.values  # pandas Series를 numpy array로 변환\nX_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n\n# 생성된 시퀀스 데이터의 형태 확인\nprint(f\"X_seq shape: {X_seq.shape}\")\nprint(f\"y_seq shape: {y_seq.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 전처리 함수 V2\ndef preprocess_data(data):\n    # 목표 변수 생성\n    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n    \n    # 피처 열만 선택\n    feature_columns = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                       'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                       'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                       'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                       'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                       'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                       'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                       'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                       'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                       'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                       'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n    # 특성과 목표 변수 분리\n    X = data[feature_columns]\n    y = data['target']\n\n    # 무한대 값을 NaN으로 대체\n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    X_imputed = imputer.fit_transform(X)\n\n    # 데이터 정규화\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X_imputed)\n\n    return X_scaled, y\n\n# 시계열 데이터 형태로 변환 함수\ndef create_sequences(data, target, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        label = target[i + sequence_length - 1]\n        sequences.append(seq)\n        targets.append(label)\n    return np.array(sequences), np.array(targets)\n\n# 데이터 전처리\nX_scaled, y = preprocess_data(data)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 데이터 길이 체크\nif len(X_scaled) < sequence_length:\n    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n\n# 시퀀스 데이터 생성\ny_array = y.values  # pandas Series를 numpy array로 변환\nX_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n\n# 생성된 시퀀스 데이터의 형태 확인\nprint(f\"X_seq shape: {X_seq.shape}\")\nprint(f\"y_seq shape: {y_seq.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**테스트 데이터 처리**","metadata":{}},{"cell_type":"code","source":"data_test_tmp = pd.read_csv(\"/kaggle/input/indicator3-test-data/SOL_Data_Test_Indicator3.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#원형\ndata_test = data_test_tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6등분 => 2개월\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4등분 => 3개월\ndef extract_last_fourth(df):\n    num_rows = len(df)\n    fourth_size = num_rows // 4\n    start_index = 3 * fourth_size\n    end_index = num_rows\n    last_fourth = df.iloc[start_index:end_index]\n    return last_fourth\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_fourth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 => 4개월\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_tmp.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 5개월\ndef extract_last_five_parts(df):\n    num_rows = len(df)\n    part_size = num_rows // 12  # 각 등분의 크기 계산\n    start_index = part_size * 7  # 맨 뒤 7등분의 시작 인덱스\n    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n    return last_five_parts\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_five_parts(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분 => 6개월\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 7개월\ndef extract_last_five_parts(df):\n    num_rows = len(df)\n    part_size = num_rows // 12  # 각 등분의 크기 계산\n    start_index = part_size * 5  # 맨 뒤 5등분의 시작 인덱스\n    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n    return last_five_parts\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_five_parts(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 마지막 등분\n# 데이터프레임의 길이 계산\ntotal_length = len(data_test)\n\n# 마지막 1/3 데이터 분리\ndata_test = data_test.iloc[-(total_length // 3):]\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 두번째\n# 데이터프레임의 길이 계산\ntotal_length = len(data_test)\n\n# 3등분으로 나눈 길이 계산\nsplit_length = total_length // 3\n\n# 중간 부분 데이터 분리\ndata_test = data_test.iloc[split_length:split_length*2]\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open_time 열을 datetime 형식으로 변환\nif not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# time 열을 분 단위로 변환\ndata_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0.1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata_test_predict = data_test.drop(columns=['open_time', 'max_return_60min', 'min_return_60min'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외 v2\n# 피처 목록\nfeatures_to_keep = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                    'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                    'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                    'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                    'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                    'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                    'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                    'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                    'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                    'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                    'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n\n# 피처들만 남기기\ndata_test_predict = data_test[features_to_keep]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n\n# 데이터 정규화\nscaler = MinMaxScaler()\ndata_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n\n# 예측 데이터를 시퀀스 형태로 변환 (LSTM용)\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TCN(시계열)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TCN 모델 정의\nclass TCNModel(nn.Module):\n    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_size) -> (batch_size, input_size, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 모델 설정\ninput_size = X_train.shape[2]\nnum_channels = 64\nmodel = TCNModel(input_size, num_channels)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install xgboost\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# XGBoost 모델 학습\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# 피처 중요도 평가\nfeature_importances = model.feature_importances_\nfeature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 피처 중요도를 데이터프레임으로 변환\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# 중요도에 따라 정렬\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n\n# 피처 중요도 표 출력\nfeature_importance_df.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SequentialFeatureSelector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 순차적 전진 선택\nmodel = LogisticRegression(max_iter=10000)\nsfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward')\nsfs.fit(X_train, y_train)\n\n# 선택된 피처\nselected_features = sfs.get_support(indices=True)\nselected_feature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns[selected_features]\n\n# 선택된 피처 중요도 (일반적으로 Logistic Regression에서는 coef_ 속성을 사용하여 피처 중요도를 평가할 수 있습니다)\nmodel.fit(sfs.transform(X_train), y_train)\nfeature_importances = model.coef_[0]\n\n# 선택된 피처 중요도를 데이터프레임으로 변환\nfeature_importance_df = pd.DataFrame({\n    'Feature': selected_feature_names,\n    'Importance': feature_importances\n})\n\n# 중요도에 따라 정렬\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n\n# 피처 중요도 표 출력\nprint(feature_importance_df)\n\n# 모델 성능 평가\naccuracy = model.score(sfs.transform(X_test), y_test)\nprint('Model accuracy with selected features:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_TCN_v4_180_11.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 테스트 데이터 텐서로 변환\nX_test_tensor = torch.tensor(X_seq, dtype=torch.float32)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/3/SOL60_SMALLL_INDICATOR3_TCN_v2.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.train()  # 모델을 학습 모드로 설정\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#추가 학습\n# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 추가 학습 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_TCN_v3.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 테스트 데이터 텐서로 변환\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_test_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/5/SOL60_SMALLL_INDICATOR3_TCN_v4.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequence_length를 사용하여 X_test_seq의 shape를 맞춤\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 예측 수행\nmodel.eval()\nwith torch.no_grad():\n    predictions = torch.sigmoid(model(X_test_tensor)).squeeze().numpy()\n\n# 예측 결과를 이진 분류로 변환 (0 또는 1)\npredictions = (predictions > 0.5).astype(int)\n\n# 예측 결과를 데이터프레임에 추가\ndata_test['prediction_Transformer'] = np.nan  # 예측 결과를 담을 열을 초기화\ndata_test.iloc[sequence_length - 1:sequence_length - 1 + len(predictions), data_test.columns.get_loc('prediction_Transformer')] = predictions\n\n# 결과 확인\nprint(data_test[['open_time', 'prediction_Transformer']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.dropna(subset=['prediction_Transformer'])\n# 결과 출력\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4등분하여 마지막 등분을 추출하는 함수\ndef extract_last_quarter(df):\n    num_rows = len(df)\n    quarter_size = num_rows // 4\n    start_index = 3 * quarter_size\n    end_index = num_rows\n    last_quarter = df.iloc[start_index:end_index]\n    return last_quarter\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_quarter(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    fifth_size = num_rows // 5\n    start_index = 4 * fifth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 5등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 7\n    start_index = 6 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['prediction_Transformer'] == 0)])\n\nprint(f\"max_return_60min이 1 이상인데 prediction이 0인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1 미만인데 prediction이 1인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 조건을 만족하는 데이터 필터링\nfiltered_data = data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 0)]\n\n# min_return_60min 값의 분포를 히스토그램으로 시각화\nplt.figure(figsize=(10, 6))\nplt.hist(filtered_data['min_return_60min'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Distribution of min_return_60min for max_return_60min >= 1.1 and prediction_Transformer == 0')\nplt.xlabel('min_return_60min')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 함수 정의: 각 행에 대해 60행 이내의 high와 low 값을 비교하여 시각을 찾는 함수\ndef find_high_low_times(df, window=60):\n    max_high_times = []\n    min_low_times = []\n\n    for i in range(len(df)):\n        # 현재 행부터 60행 이내의 데이터를 선택\n        subset = df.iloc[i:i+window]\n\n        # 최대 high와 최소 low 값을 찾음\n        max_high_idx = subset['high'].idxmax()\n        min_low_idx = subset['low'].idxmin()\n\n        # 해당 인덱스의 시간을 저장\n        max_high_time = df.at[max_high_idx, 'open_time']\n        min_low_time = df.at[min_low_idx, 'open_time']\n\n        max_high_times.append(max_high_time)\n        min_low_times.append(min_low_time)\n\n    df['max_high_time'] = max_high_times\n    df['min_low_time'] = min_low_times\n\n    return df\n\n# 함수 호출\ndata_test = find_high_low_times(data_test)\ndata_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 조건에 맞는 새로운 열 추가\ndef add_condition_column(df):\n    # 'max_high_time'과 'min_low_time'이 datetime 형식인지 확인하고 변환\n    df['max_high_time'] = pd.to_datetime(df['max_high_time'])\n    df['min_low_time'] = pd.to_datetime(df['min_low_time'])\n    \n    # 조건에 맞는 행 필터링 (max_return_60min >= 1.1 및 prediction_Transformer == 1)\n    filtered_df = df[(df['max_return_60min'] >= 1.1) & (df['prediction_Transformer'] == 1)].copy()\n    \n    # 새로운 열 추가 및 초기화\n    filtered_df['condition'] = 0\n    \n    # 조건을 만족하는 경우 condition 값을 설정\n    condition_indices = filtered_df.index[filtered_df['max_high_time'] < filtered_df['min_low_time']]\n    \n    filtered_df.loc[condition_indices, 'condition'] = 1\n    \n    return filtered_df\n\n# 데이터 타입 변환\ndata_test['max_high_time'] = pd.to_datetime(data_test['max_high_time'])\ndata_test['min_low_time'] = pd.to_datetime(data_test['min_low_time'])\n\n# 조건 열 추가\nfiltered_data_test = add_condition_column(data_test)\n\n# 결과 확인\nfiltered_data_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"condition_1_data = filtered_data_test[filtered_data_test['condition'] == 0]\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_high_time과 min_low_time의 차이를 분으로 계산하여 새로운 열 추가\ncondition_1_data['time_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['min_low_time']).dt.total_seconds() / 60\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#min_low_time과 open_time의 차이를 분으로 계산하여 새로운 열 추가\ncondition_1_data['min_low_open_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['open_time']).dt.total_seconds() / 60\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\ntime_difference_max = condition_1_data['min_low_open_difference_minutes'].max()\ntime_difference_min = condition_1_data['min_low_open_difference_minutes'].min()\ntime_difference_mode = condition_1_data['min_low_open_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\ntime_difference_mean = condition_1_data['min_low_open_difference_minutes'].mean()\n\n# 결과 출력\ntime_difference_stats = {\n    'max': time_difference_max,\n    'min': time_difference_min,\n    'mode': time_difference_mode, #최빈\n    'mean': time_difference_mean # 평균\n}\n\ntime_difference_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_return_60min이 -1 이하인 행들 필터링\nnegative_return_data = condition_1_data[condition_1_data['min_return_60min'] <= -1]\n\n# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\ntime_difference_max = negative_return_data['time_difference_minutes'].max()\ntime_difference_min = negative_return_data['time_difference_minutes'].min()\ntime_difference_mode = negative_return_data['time_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\ntime_difference_mean = negative_return_data['time_difference_minutes'].mean()\n\n# 결과 출력\ntime_difference_stats = {\n    'max': time_difference_max,\n    'min': time_difference_min,\n    'mode': time_difference_mode,  # 최빈값\n    'mean': time_difference_mean   # 평균값\n}\n\ntime_difference_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 모델 설정\ninput_dim = X_train.shape[2]\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 학습률 감소 스케줄러 설정\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# 조기 종료 설정\npatience = 5\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 모델 저장 경로\nbest_model_path = \"best_transformer_model.pth\"\n\n# 모델 학습\nnum_epochs = 20  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Duration: {epoch_duration:.2f}s')\n\n    # 최적의 모델 저장 및 조기 종료 조건 체크\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Best model saved with val loss: {best_val_loss:.4f}\")\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n    # 학습률 감소\n    scheduler.step()\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 최적의 모델 로드\nmodel.load_state_dict(torch.load(best_model_path))\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install ipywidgets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 입력 차원 확인 및 설정\ninput_dim = X_train.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/6/SOL60_SMALLL_INDICATOR3_Transformer_v3.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.train()  # 추가 학습을 위해 학습 모드로 전환\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#추가학습\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 모델 정의 (로드할 때 필요)\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 입력 차원 확인 및 설정\ninput_dim = X_test_seq.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/5/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequence_length를 사용하여 X_test_seq의 shape를 맞춤\nsequence_length = X_test_seq.shape[1]\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).view(-1, sequence_length, input_dim)\n\n# 예측 수행\nwith torch.no_grad():\n    predictions = torch.sigmoid(model(X_test_tensor)).squeeze().numpy()\n\n# 예측 결과를 이진 분류로 변환 (0 또는 1)\npredictions = (predictions > 0.5).astype(int)\n\n# 예측 결과를 데이터프레임에 추가\ndata_test['prediction_Transformer'] = np.nan  # 예측 결과를 담을 열을 초기화\ndata_test.iloc[sequence_length - 1:sequence_length - 1 + len(predictions), data_test.columns.get_loc('prediction_Transformer')] = predictions\n\n# 결과 확인\nprint(data_test[['open_time', 'prediction_Transformer']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_non_nan = data_test.dropna(subset=['prediction_Transformer'])\n\n# max_return_60min 기준으로 내림차순 정렬\ndata_test_sorted = data_test_non_nan.sort_values(by='max_return_60min', ascending=False)\n\n# 결과 출력\ndata_test_sorted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4등분하여 마지막 등분을 추출하는 함수\ndef extract_last_quarter(df):\n    num_rows = len(df)\n    quarter_size = num_rows // 4\n    start_index = 3 * quarter_size\n    end_index = num_rows\n    last_quarter = df.iloc[start_index:end_index]\n    return last_quarter\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_quarter(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    fifth_size = num_rows // 5\n    start_index = 4 * fifth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 5등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 7\n    start_index = 6 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['prediction_Transformer'] == 0)])\n\nprint(f\"max_return_60min이 1 이상인데 prediction이 0인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1 미만인데 prediction이 1인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])\n        return output\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        running_loss += loss.item()\n    avg_train_loss = running_loss / len(train_loader)\n    return avg_train_loss\n\ndef validate_model(model, test_loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n    avg_val_loss = val_loss / len(test_loader)\n    return avg_val_loss\n\ndef _mp_fn(rank, flags):\n    # 데이터 로더 생성\n    train_loader = pl.MpDeviceLoader(train_dataset, xm.xla_device())\n    test_loader = pl.MpDeviceLoader(test_dataset, xm.xla_device())\n    \n    input_dim = X_train.shape[2]\n    nhead = 2\n    num_layers = 2\n    dim_feedforward = 64\n    output_dim = 1\n\n    model = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim).to(xm.xla_device())\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n    patience = 5\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    train_losses = []\n    val_losses = []\n\n    num_epochs = 50\n    print_freq = 5\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        avg_train_loss = train_model(model, train_loader, criterion, optimizer, xm.xla_device())\n        train_losses.append(avg_train_loss)\n        \n        avg_val_loss = validate_model(model, test_loader, criterion, xm.xla_device())\n        val_losses.append(avg_val_loss)\n\n        end_time = time.time()\n        epoch_duration = end_time - start_time\n\n        if (epoch + 1) % print_freq == 0 or epoch == num_epochs - 1:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Duration: {epoch_duration:.2f}s')\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            xm.save(model.state_dict(), 'best_transformer_model.pth')\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n            break\n\n        scheduler.step()\n\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('training_validation_loss.png')  # 학습 곡선 저장\n    plt.show()\n\n    # 모델을 반환하기 위해 로드하고 저장 경로 설정\n    model.load_state_dict(torch.load('best_transformer_model.pth'))\n    model.to(xm.xla_device())\n\n    model.eval()\n    with torch.no_grad():\n        y_true = []\n        y_pred = []\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(xm.xla_device()), y_batch.to(xm.xla_device())\n            output = model(X_batch)\n            y_true.extend(y_batch.tolist())\n            y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n    y_pred = np.array(y_pred) > 0.5\n    y_true = y_test_tensor.numpy()\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n    print(f'F1 Score: {f1:.4f}')\n\n    # 혼동 행렬 시각화\n    plt.figure(figsize=(6, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')  # 혼동 행렬 저장\n    plt.show()\n\n    # 모델 저장 경로\n    model_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v4.pth'\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n\n    return model  # 모델 반환\n\nFLAGS = {}\nmodel = xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v4.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"시간대 분석","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n\n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n\n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_imputed = imputer.fit_transform(data)\n\n# numpy 배열을 다시 DataFrame으로 변환\ndata = pd.DataFrame(data_imputed, columns=data.columns)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nresults = results.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(results['time'], results['percentage_above_1_1'], marker='o', linestyle='-')\nplt.title('Percentage of max_return_60min >= 1.1 over Time')\nplt.xlabel('Time')\nplt.ylabel('Percentage of max_return_60min >= 1.1')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 값을 시-분 형식으로 변환하는 함수\ndef convert_minutes_to_time(minutes):\n    hours = minutes // 60\n    mins = minutes % 60\n    return f\"{int(hours):02}:{int(mins):02}\"\n\n# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# time 값을 시-분 형식으로 변환\nresults = results.reset_index()\nresults['time'] = results['time'].apply(convert_minutes_to_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nresults = results.reset_index()\n\n# 슬라이딩 윈도우의 크기 설정 (예: 60분)\nwindow_size = 60\n\n# 슬라이딩 윈도우 평균 계산\nresults['rolling_mean'] = results['percentage_above_1_1'].rolling(window=window_size // 10, min_periods=1).mean()\n\n# 임계값 설정\nthreshold = 33  # 여기서 임계값을 설정하세요\n\n# 임계값 이상의 구간 필터링\nhigh_prob_zones = results[results['rolling_mean'] >= threshold]\n\n# 연속된 구간의 시작과 끝을 식별\nhigh_prob_zones['group'] = (high_prob_zones['time'].diff() > 10).cumsum()\n\n# 각 그룹의 시작과 끝 시간 구하기\ngrouped_zones = high_prob_zones.groupby('group').agg(\n    start_time=('time', 'first'),\n    end_time=('time', 'last')\n).reset_index()\n\n# 시간축 이동\nshift_point = 200\nbefore_shift = results[results['time'] < shift_point].copy()\nafter_shift = results[results['time'] >= shift_point].copy()\n\nbefore_shift['shifted_time'] = before_shift['time'] + 1440\nafter_shift['shifted_time'] = after_shift['time']\n\nresults_shifted = pd.concat([after_shift, before_shift]).reset_index(drop=True)\n\n# 시각화\nplt.figure(figsize=(12, 6))\nplt.plot(results_shifted['shifted_time'], results_shifted['percentage_above_1_1'], marker='o', linestyle='-', label='Percentage over Time')\nplt.plot(results_shifted['shifted_time'], results_shifted['rolling_mean'], linestyle='-', color='orange', label='Rolling Mean')\n\n# 임계값 선 추가\nplt.axhline(y=threshold, color='purple', linestyle='-', label=f'Threshold {threshold}%')\n\n# 영역 색칠\nplt.fill_between(results_shifted['shifted_time'], threshold, results_shifted['percentage_above_1_1'], where=(results_shifted['percentage_above_1_1'] >= threshold), interpolate=True, color='purple', alpha=0.3)\n\n# 높은 확률 구간에서 시작과 끝 구간만 표시\nfor _, row in grouped_zones.iterrows():\n    if row['start_time'] < shift_point:\n        shifted_start_time = row['start_time'] + 1440\n    else:\n        shifted_start_time = row['start_time']\n    if row['end_time'] < shift_point:\n        shifted_end_time = row['end_time'] + 1440\n    else:\n        shifted_end_time = row['end_time']\n    plt.axvline(x=shifted_start_time, color='red', linestyle='--', label='Start Time' if row['start_time'] == grouped_zones['start_time'].iloc[0] else \"\")\n    plt.axvline(x=shifted_end_time, color='green', linestyle='--', label='End Time' if row['end_time'] == grouped_zones['end_time'].iloc[0] else \"\")\n\n# 0 지점을 나타내는 세로선 추가\nplt.axvline(x=shift_point, color='blue', linestyle=':', label='Midnight Transition')\n\nplt.title('Percentage of max_return_60min >= 1.1 over Time')\nplt.xlabel('Time (minutes)')\nplt.ylabel('Percentage of max_return_60min >= 1.1')\nplt.xticks(rotation=45)\nplt.xlim(shift_point, 1440 + shift_point)\nplt.xticks(np.arange(shift_point, 1440 + shift_point + 1, 100), labels=[str(x % 1440) for x in np.arange(shift_point, 1440 + shift_point + 1, 100)])\nplt.xticks(list(plt.xticks()[0]) + [0], labels=list(plt.xticks()[1]) + ['0'])  # 0을 x축에 추가\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 분을 시-분 형식으로 변환하는 함수\ndef convert_minutes_to_time(minutes):\n    hours = minutes // 60\n    mins = minutes % 60\n    return f\"{int(hours):02}:{int(mins):02}\"\n\n# 한국 시간으로 변환하는 함수\ndef convert_to_kst(utc_minutes):\n    kst_minutes = (utc_minutes + 540) % 1440  # 한국 시간은 UTC+9 (540분)\n    return convert_minutes_to_time(kst_minutes)\n\n# 높은 확률 구간 시작과 끝 값 출력\nprint(f\"High probability zones above {threshold}% (start_time, end_time):\")\nfor _, row in grouped_zones.iterrows():\n    start_time_utc = convert_minutes_to_time(row['start_time'])\n    end_time_utc = convert_minutes_to_time(row['end_time'])\n    start_time_kst = convert_to_kst(row['start_time'])\n    end_time_kst = convert_to_kst(row['end_time'])\n    print(f\"Start: {start_time_utc} (KST: {start_time_kst}), End: {end_time_utc} (KST: {end_time_kst})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"요일분석","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n        \n    # 요일 추출하여 data_day 변수에 저장\n    data_day = data['open_time'].dt.day_name()\n    \n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute    \n    \n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    # 무한대 값을 NaN으로 대체\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    data_imputed = imputer.fit_transform(data)\n\n    # numpy 배열을 다시 DataFrame으로 변환\n    data = pd.DataFrame(data_imputed, columns=data.columns)\n    \n    data['day'] = data_day\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 그룹화하여 집계 계산\nresults = data.groupby('day').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 요일 순서 정렬\nordered_days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nresults = results.reindex(ordered_days)\n\n# 결과 출력\nprint(results)\n\n# 시각화\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# 막대 그래프 생성\nresults['percentage_above_1_1'].plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n\n# 그래프 제목 및 축 레이블 설정\nax.set_title('Percentage of max_return_60min >= 1.1 by Day of Week')\nax.set_xlabel('Day of Week')\nax.set_ylabel('Percentage (%)')\nax.set_xticklabels(ordered_days, rotation=45, ha='right')\n\n# 값 레이블 추가\nfor i in ax.containers:\n    ax.bar_label(i, fmt='%.1f%%')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"변동에 대한 최대 하락 구간의 분포","metadata":{}},{"cell_type":"code","source":"pip install mplfinance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport mplfinance as mpf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n        \n    # 요일 추출하여 data_day 변수에 저장\n    data_day = data['open_time'].dt.day_name()\n    \n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute    \n    \n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    # 무한대 값을 NaN으로 대체\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    data_imputed = imputer.fit_transform(data)\n\n    # numpy 배열을 다시 DataFrame으로 변환\n    data = pd.DataFrame(data_imputed, columns=data.columns)\n    \n    data['day'] = data_day\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\n\n# 시간 변환 함수\ndef convert_time_to_hhmm(df, time_col='time'):\n    # 분 단위를 시-분 형식으로 변환\n    df['hhmm_time'] = pd.to_datetime(df[time_col], unit='m').dt.strftime('%H:%M')\n    return df\n\n# 시간 변환 함수 적용\ndata = convert_time_to_hhmm(data)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min이 1.1 이상인 데이터 필터링\nfiltered_data = data[data['max_return_60min'] >= 1.5]\n\n# hhmm_time을 datetime 형식으로 변환\nfiltered_data['hhmm_time'] = pd.to_datetime(filtered_data['hhmm_time'], format='%H:%M')\n\n# hhmm_time 열을 기준으로 그룹화하여 min_return_60min의 분포를 캔들 스틱으로 표시\ncandlestick_data = filtered_data.groupby(filtered_data['hhmm_time']).agg(\n    Open=('min_return_60min', 'first'),\n    High=('min_return_60min', 'max'),\n    Low=('min_return_60min', 'min'),\n    Close=('min_return_60min', 'last')\n)\n\n# 인덱스를 DatetimeIndex로 설정\ncandlestick_data.index = pd.to_datetime(candlestick_data.index)\n\n# 캔들 스틱 차트 시각화\nmpf.plot(candlestick_data, type='candle', style='charles', title='Candlestick chart of min_return_60min', ylabel='min_return_60min')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min과 min_return_60min 열을 마지막 부분으로 이동\ncolumns = list(data.columns)\ncolumns.remove('max_return_60min')\ncolumns.remove('min_return_60min')\ncolumns.extend(['max_return_60min', 'min_return_60min'])\ndata = data[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min이 1.1 이상이면서 min_return_60min이 1 이하인 데이터 필터링\nfiltered_data2 = data[(data['max_return_60min'] >= 1.5) & (data['min_return_60min'] <= -10)]\n\n# 결과 출력\nfiltered_data2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###########################Trailing Test######################","metadata":{}},{"cell_type":"code","source":"#테스트 데이터 호출\ndata_test_tmp = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL_Data_Test_Indicator3.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#테스트 데이터 호출\ndata_test_tmp = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL_Data_Test_Indicator3_180.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:38.962831Z","iopub.execute_input":"2024-08-07T13:22:38.963644Z","iopub.status.idle":"2024-08-07T13:22:53.455815Z","shell.execute_reply.started":"2024-08-07T13:22:38.963607Z","shell.execute_reply":"2024-08-07T13:22:53.454990Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"             open_time   open   high    low  close   volume   time  atr_5  \\\n0  2023-07-26 15:00:00  24.49  24.53  24.49  24.53  4241.87  15:00  0.000   \n1  2023-07-26 15:01:00  24.53  24.57  24.53  24.55  5665.44  15:01  0.000   \n2  2023-07-26 15:02:00  24.55  24.59  24.54  24.57  3515.11  15:02  0.000   \n3  2023-07-26 15:03:00  24.58  24.58  24.53  24.53  2160.81  15:03  0.000   \n4  2023-07-26 15:04:00  24.53  24.55  24.52  24.53  2068.28  15:04  0.042   \n\n   atr_10  atr_14  ...  price_ma_20  volume_ma_20  price_ma_50  volume_ma_50  \\\n0     0.0     0.0  ...          NaN           NaN          NaN           NaN   \n1     0.0     0.0  ...          NaN           NaN          NaN           NaN   \n2     0.0     0.0  ...          NaN           NaN          NaN           NaN   \n3     0.0     0.0  ...          NaN           NaN          NaN           NaN   \n4     0.0     0.0  ...          NaN           NaN          NaN           NaN   \n\n   price_ma_100  volume_ma_100  price_ma_200  volume_ma_200  max_return_60min  \\\n0           NaN            NaN           NaN            NaN          1.916021   \n1           NaN            NaN           NaN            NaN          1.832994   \n2           NaN            NaN           NaN            NaN          1.750102   \n3           NaN            NaN           NaN            NaN          1.916021   \n4           NaN            NaN           NaN            NaN          1.916021   \n\n   min_return_60min  \n0         -0.122299  \n1         -0.203666  \n2         -0.284900  \n3         -0.122299  \n4         -0.122299  \n\n[5 rows x 139 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open_time</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>time</th>\n      <th>atr_5</th>\n      <th>atr_10</th>\n      <th>atr_14</th>\n      <th>...</th>\n      <th>price_ma_20</th>\n      <th>volume_ma_20</th>\n      <th>price_ma_50</th>\n      <th>volume_ma_50</th>\n      <th>price_ma_100</th>\n      <th>volume_ma_100</th>\n      <th>price_ma_200</th>\n      <th>volume_ma_200</th>\n      <th>max_return_60min</th>\n      <th>min_return_60min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-07-26 15:00:00</td>\n      <td>24.49</td>\n      <td>24.53</td>\n      <td>24.49</td>\n      <td>24.53</td>\n      <td>4241.87</td>\n      <td>15:00</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.916021</td>\n      <td>-0.122299</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-07-26 15:01:00</td>\n      <td>24.53</td>\n      <td>24.57</td>\n      <td>24.53</td>\n      <td>24.55</td>\n      <td>5665.44</td>\n      <td>15:01</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.832994</td>\n      <td>-0.203666</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-07-26 15:02:00</td>\n      <td>24.55</td>\n      <td>24.59</td>\n      <td>24.54</td>\n      <td>24.57</td>\n      <td>3515.11</td>\n      <td>15:02</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.750102</td>\n      <td>-0.284900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-07-26 15:03:00</td>\n      <td>24.58</td>\n      <td>24.58</td>\n      <td>24.53</td>\n      <td>24.53</td>\n      <td>2160.81</td>\n      <td>15:03</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.916021</td>\n      <td>-0.122299</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-07-26 15:04:00</td>\n      <td>24.53</td>\n      <td>24.55</td>\n      <td>24.52</td>\n      <td>24.53</td>\n      <td>2068.28</td>\n      <td>15:04</td>\n      <td>0.042</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.916021</td>\n      <td>-0.122299</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 139 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_test = data_test_tmp\ndata_test.columns","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:53.457207Z","iopub.execute_input":"2024-08-07T13:22:53.457656Z","iopub.status.idle":"2024-08-07T13:22:53.463103Z","shell.execute_reply.started":"2024-08-07T13:22:53.457620Z","shell.execute_reply":"2024-08-07T13:22:53.462361Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Index(['open_time', 'open', 'high', 'low', 'close', 'volume', 'time', 'atr_5',\n       'atr_10', 'atr_14',\n       ...\n       'price_ma_20', 'volume_ma_20', 'price_ma_50', 'volume_ma_50',\n       'price_ma_100', 'volume_ma_100', 'price_ma_200', 'volume_ma_200',\n       'max_return_60min', 'min_return_60min'],\n      dtype='object', length=139)"},"metadata":{}}]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open_time 열을 datetime 형식으로 변환\nif not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# time 열을 분 단위로 변환\ndata_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:53.464057Z","iopub.execute_input":"2024-08-07T13:22:53.464320Z","iopub.status.idle":"2024-08-07T13:22:53.628419Z","shell.execute_reply.started":"2024-08-07T13:22:53.464294Z","shell.execute_reply":"2024-08-07T13:22:53.627563Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata_test_predict = data_test.drop(columns=['open_time', 'max_return_60min', 'min_return_60min'])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:53.630271Z","iopub.execute_input":"2024-08-07T13:22:53.630558Z","iopub.status.idle":"2024-08-07T13:22:53.797797Z","shell.execute_reply.started":"2024-08-07T13:22:53.630518Z","shell.execute_reply":"2024-08-07T13:22:53.796922Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외 v2\n# 피처 목록\nfeatures_to_keep = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                    'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                    'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                    'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                    'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                    'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                    'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                    'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                    'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                    'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                    'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n\n# 피처들만 남기기\ndata_test_predict = data_test[features_to_keep]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n\n# 데이터 정규화\nscaler = MinMaxScaler()\ndata_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:53.798921Z","iopub.execute_input":"2024-08-07T13:22:53.799225Z","iopub.status.idle":"2024-08-07T13:23:05.560690Z","shell.execute_reply.started":"2024-08-07T13:22:53.799189Z","shell.execute_reply":"2024-08-07T13:23:05.559784Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#Transformer model 호출\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# 모델 정의 (로드할 때 필요)\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformer model 호출2\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측 데이터를 시퀀스 형태로 변환\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n\n# 입력 차원 확인 및 설정\ninput_dim = X_test_seq.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/6/SOL60_SMALLL_INDICATOR3_Transformer_v3.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TCN 모델 호출\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:22:02.959628Z","iopub.execute_input":"2024-08-07T13:22:02.959980Z","iopub.status.idle":"2024-08-07T13:22:02.966434Z","shell.execute_reply.started":"2024-08-07T13:22:02.959951Z","shell.execute_reply":"2024-08-07T13:22:02.965723Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#TCN 모델 호출2\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측 데이터를 시퀀스 형태로 변환\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_test_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/9/SOL60_SMALLL_INDICATOR3_TCN_v4_180_11.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:23:05.561928Z","iopub.execute_input":"2024-08-07T13:23:05.562284Z","iopub.status.idle":"2024-08-07T13:23:24.325724Z","shell.execute_reply.started":"2024-08-07T13:23:05.562250Z","shell.execute_reply":"2024-08-07T13:23:24.324819Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Input channels: 136\nModel loaded from /kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/9/SOL60_SMALLL_INDICATOR3_TCN_v4_180_11.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# 슬라이딩 윈도우로 데이터 범위 추출\nnum_rows = data_test_predict_scaled.shape[0]\n\n# 시퀀스 데이터를 텐서로 변환\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\nprint(f\"Tensor Transform Complete\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:23:29.343125Z","iopub.execute_input":"2024-08-07T13:23:29.343465Z","iopub.status.idle":"2024-08-07T13:23:31.920842Z","shell.execute_reply.started":"2024-08-07T13:23:29.343435Z","shell.execute_reply":"2024-08-07T13:23:31.919935Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Tensor Transform Complete\n","output_type":"stream"}]},{"cell_type":"code","source":"# 모델을 GPU로 이동 (가능한 경우)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nX_test_tensor = X_test_tensor.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:23:31.922258Z","iopub.execute_input":"2024-08-07T13:23:31.922579Z","iopub.status.idle":"2024-08-07T13:23:31.927230Z","shell.execute_reply.started":"2024-08-07T13:23:31.922525Z","shell.execute_reply":"2024-08-07T13:23:31.926474Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#prediction 결과 저장\nresults = []\n\n#window_size = 216000\n#window_size = 259200\nwindow_size = 302400\n\nbatch_size = 63  # 배치 크기 설정\n\nfor end in tqdm(range(num_rows, window_size - 1, -batch_size)):\n    start = max(end - batch_size + 1, 0)\n    \n    # 해당 범위에 대한 시퀀스 텐서 추출\n    X_test_tensor_tmp = X_test_tensor[start:end]\n    \n    # 예측 수행\n    model.eval()\n    with torch.no_grad():\n        predictions = torch.sigmoid(model(X_test_tensor_tmp)).squeeze().cpu().numpy()\n\n    # 예측 결과를 이진 분류로 변환 (0 또는 1)\n    predictions = (predictions > 0.5).astype(int)\n    \n    # 예측 결과의 마지막 값을 추가\n    if len(predictions.shape) > 0:\n        results.append(predictions[-1])\n    else:\n        results.append(predictions)\n\nresults = results[::-1]  # 원래 순서대로 변경","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:25:12.422013Z","iopub.execute_input":"2024-08-07T13:25:12.422371Z","iopub.status.idle":"2024-08-07T13:25:25.626110Z","shell.execute_reply.started":"2024-08-07T13:25:12.422341Z","shell.execute_reply":"2024-08-07T13:25:25.625234Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"100%|██████████| 1827/1827 [00:13<00:00, 138.47it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 결과를 원본 데이터프레임에 추가\ndata_test['Predictions'] = np.nan\ndata_test.loc[data_test.index[-len(results):], 'Predictions'] = results\ndata_test = data_test.dropna(subset=['Predictions'])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:25:25.627626Z","iopub.execute_input":"2024-08-07T13:25:25.627955Z","iopub.status.idle":"2024-08-07T13:25:25.637157Z","shell.execute_reply.started":"2024-08-07T13:25:25.627921Z","shell.execute_reply":"2024-08-07T13:25:25.636375Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['Predictions'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['Predictions'] == 0)])\n    \n#\nprint(f\"max_return_60min이 1.1 이상인데 prediction이 1인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1.1 미만인데 prediction이 0인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T13:25:25.638182Z","iopub.execute_input":"2024-08-07T13:25:25.638895Z","iopub.status.idle":"2024-08-07T13:25:25.656721Z","shell.execute_reply.started":"2024-08-07T13:25:25.638851Z","shell.execute_reply":"2024-08-07T13:25:25.655942Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"max_return_60min이 1.1 이상인데 prediction이 1인 데이터의 비율: 10.892172961138478\nmax_return_60min이 1.1 미만인데 prediction이 0인 데이터의 비율: 44.991789819376024\n","output_type":"stream"}]},{"cell_type":"code","source":"# 시간대를 나타내는 새로운 열 추가 (30분 단위)\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# 시간대별로 데이터 그룹화 (30분 단위)\ngrouped = data_test.groupby('half_hour')\n\n# 각 시간대별로 조건을 만족하는 데이터의 개수를 계산하는 함수\ndef calculate_counts(group):\n    count_max_return_ge_1_prediction_0 = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    count_max_return_lt_1_prediction_1 = len(group[(group['max_return_60min'] < 1.1) & (group['Predictions'] == 0)])\n    \n    total_count = len(group)\n    \n    if total_count == 0:\n        return pd.Series([0, 0, 0, 0, 0])\n    \n    rate_max_return_ge_1_prediction_0 = (count_max_return_ge_1_prediction_0 / total_count) * 100\n    rate_max_return_lt_1_prediction_1 = (count_max_return_lt_1_prediction_1 / total_count) * 100\n    \n    total_rate = rate_max_return_ge_1_prediction_0 + rate_max_return_lt_1_prediction_1\n    \n    return pd.Series([count_max_return_ge_1_prediction_0, count_max_return_lt_1_prediction_1, rate_max_return_ge_1_prediction_0, rate_max_return_lt_1_prediction_1, total_rate])\n\n# 각 그룹에 함수 적용\nresults = grouped.apply(calculate_counts)\nresults.columns = ['Count_GE_1_Pred_0', 'Count_LT_1_Pred_1', 'Rate_GE_1_Pred_0', 'Rate_LT_1_Pred_1', 'Total_Rate']\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 UTC 및 KST로 표시\nresults['half_hour_utc'] = pd.to_datetime(results.index.astype(str), format='%H:%M:%S').time\nresults['half_hour_kst'] = (pd.to_datetime(results.index.astype(str), format='%H:%M:%S') + pd.Timedelta(hours=9)).time\nresults['half_hour_label'] = results.index.astype(str) + ' (' + results['half_hour_kst'].astype(str) + ' KST)'\n\n# x축을 시간으로, y축을 Total_Rate으로 하는 차트 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(results['half_hour_label'], results['Total_Rate'], marker='o')\nplt.xlabel('Half Hour of the Day (UTC)')\nplt.ylabel('Total Rate (%)')\nplt.title('Total Rate of Conditions Met by Half Hour of the Day (UTC with KST)')\nplt.grid(True)\nplt.xticks(rotation=45, ha='right')  # 레이블을 오른쪽으로 회전하여 수평으로 정렬\n\n# y축 90 라인 진하게 표시\nplt.axhline(90, color='red', linewidth=1.5, linestyle='--')  # y=90 라인을 진하게\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 나타내는 새로운 열 추가 (30분 단위)\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# 시간대별로 데이터 그룹화 (30분 단위)\ngrouped = data_test.groupby('half_hour')\n\n# 각 시간대별로 max_return_60min이 1.1 이상이고 Predictions가 1인 값의 비율을 계산하는 함수\ndef calculate_ratio(group):\n    count_condition_met = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    total_count = len(group)\n    \n    if total_count == 0:\n        return 0\n    \n    return (count_condition_met / total_count) * 100\n\n# 각 그룹에 함수 적용\nresults = grouped.apply(calculate_ratio)\nresults = results.reset_index()\nresults.columns = ['half_hour', 'Condition_Met_Ratio']\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 UTC 및 KST로 표시\nresults['half_hour'] = pd.to_datetime(results['half_hour'].astype(str), format='%H:%M:%S').dt.strftime('%H:%M')\nresults['half_hour_kst'] = (pd.to_datetime(results['half_hour'], format='%H:%M') + pd.Timedelta(hours=9)).dt.strftime('%H:%M')\nresults['half_hour_label'] = results['half_hour'] + ' (' + results['half_hour_kst'] + ' KST)'\n\n# x축을 시간으로, y축을 Condition_Met_Ratio로 하는 차트 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(results['half_hour_label'], results['Condition_Met_Ratio'], marker='o')\nplt.xlabel('Half Hour of the Day (UTC)')\nplt.ylabel('Condition Met Ratio (%)')\nplt.title('Ratio of max_return_60min >= 1.1 and Predictions == 1 by Half Hour (UTC with KST)')\nplt.grid(True)\nplt.xticks(rotation=45, ha='right')  # 레이블을 오른쪽으로 회전하여 수평으로 정렬\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'open_time' to datetime if it's not already\ndata_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# Add a new column representing half-hour intervals\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# Group by the half-hour intervals\ngrouped = data_test.groupby('half_hour')\n\n# Define the function to calculate the counts and rates\ndef calculate_counts(group):\n    count_max_return_ge_1_prediction_0 = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    count_max_return_lt_1_prediction_1 = len(group[(group['max_return_60min'] < 1.1) & (group['Predictions'] == 0)])\n    \n    total_count = len(group)\n    \n    if total_count == 0:\n        return pd.Series([0, 0, 0, 0, 0])\n    \n    rate_max_return_ge_1_prediction_0 = (count_max_return_ge_1_prediction_0 / total_count) * 100\n    rate_max_return_lt_1_prediction_1 = (count_max_return_lt_1_prediction_1 / total_count) * 100\n    \n    total_rate = rate_max_return_ge_1_prediction_0 + rate_max_return_lt_1_prediction_1\n    \n    return pd.Series([count_max_return_ge_1_prediction_0, count_max_return_lt_1_prediction_1, rate_max_return_ge_1_prediction_0, rate_max_return_lt_1_prediction_1, total_rate])\n\n# Apply the function to the grouped data\nresults = grouped.apply(calculate_counts)\nresults.columns = ['Count_GE_1_Pred_0', 'Count_LT_1_Pred_1', 'Rate_GE_1_Pred_0', 'Rate_LT_1_Pred_1', 'Total_Rate']\n\n# Function to calculate the ratio for condition met\ndef calculate_ratio(group):\n    count_condition_met = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    total_count = len(group)\n    \n    if total_count == 0:\n        return 0\n    \n    return (count_condition_met / total_count) * 100\n\n# Apply the function to the grouped data\nratio_results = grouped.apply(calculate_ratio)\nratio_results = ratio_results.reset_index()\nratio_results.columns = ['half_hour', 'Condition_Met_Ratio']\n\n# Combine the results into a single DataFrame\ncombined_results = results.reset_index()\ncombined_results['Condition_Met_Ratio'] = ratio_results['Condition_Met_Ratio']\n\n# 시간대를 UTC 및 KST로 표시\ncombined_results['half_hour_utc'] = pd.to_datetime(combined_results['half_hour'].astype(str), format='%H:%M:%S').dt.strftime('%H:%M')\ncombined_results['half_hour_kst'] = (pd.to_datetime(combined_results['half_hour_utc'], format='%H:%M') + pd.Timedelta(hours=9)).dt.strftime('%H:%M')\ncombined_results['half_hour_label'] = combined_results['half_hour_utc'] + ' (' + combined_results['half_hour_kst'] + ' KST)'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the combined results\nfig, ax1 = plt.subplots(figsize=(12, 6))\n\nax1.plot(combined_results['half_hour_label'], combined_results['Total_Rate'], marker='o', color='b', label='Total Rate')\nax1.set_xlabel('Half Hour of the Day (UTC)')\nax1.set_ylabel('Total Rate (%)', color='b')\nax1.tick_params(axis='y', labelcolor='b')\nax1.grid(True)\nplt.xticks(rotation=45, ha='right')\n\nax2 = ax1.twinx()\nax2.plot(combined_results['half_hour_label'], combined_results['Condition_Met_Ratio'], marker='o', color='g', label='Condition Met Ratio')\nax2.set_ylabel('Condition Met Ratio (%)', color='g')\nax2.tick_params(axis='y', labelcolor='g')\n\nfig.tight_layout()\nplt.title('Comparison of Total Rate and Condition Met Ratio by Half Hour (UTC with KST)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}