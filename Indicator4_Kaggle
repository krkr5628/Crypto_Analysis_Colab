{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9122986,"sourceType":"datasetVersion","datasetId":5507351},{"sourceId":9145059,"sourceType":"datasetVersion","datasetId":5519552},{"sourceId":90787,"sourceType":"modelInstanceVersion","modelInstanceId":69639,"modelId":94706}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install imbalanced-learn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tdqm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport logging\n\n# TensorFlow 및 TPU 설정\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.impute import SimpleImputer\nimport joblib\nimport matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\n\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**학습 데이터 처리**","metadata":{}},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터2\ndata = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL60_INDICATOR3_180\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 두번째\n# 데이터프레임의 길이 계산\ntotal_length = len(data)\n\n# 3등분으로 나눈 길이 계산\nsplit_length = total_length // 3\n\n# 중간 부분 데이터 분리\ndata = data.iloc[split_length:split_length*2]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 마지막 등분\n# 데이터프레임의 길이 계산\ntotal_length = len(data)\n\n# 마지막 1/3 데이터 분리\ndata = data.iloc[-(total_length // 3):]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows_2_3 = int(len(data) * 2 / 3)\n\n# Extract the first 2/3 of the dataset\ndata = data.iloc[:rows_2_3]\n\n# Display the first few rows of the extracted dataset to ensure it is correct\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-test/SOL60_INDICATOR3_SMALL\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata = data.drop(columns=['Unnamed: 0.1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n\n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n\n    # 사용하지 않을 열 제외\n    #data = data.drop(columns=['open_time', 'Unnamed: 0'])\n    data = data.drop(columns=['open_time'])\n    \n    return data\n\n# 시간 관련 열 변환\ndata = convert_time_features(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 전처리 함수\ndef preprocess_data(data):\n    # 목표 변수 생성\n    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n\n    # 특성과 목표 변수 분리\n    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n    y = data['target']\n\n    # 무한대 값을 NaN으로 대체\n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    X_imputed = imputer.fit_transform(X)\n\n    # 데이터 정규화\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X_imputed)\n\n    return X_scaled, y\n\n# 시계열 데이터 형태로 변환 함수\ndef create_sequences(data, target, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        label = target[i + sequence_length - 1]\n        sequences.append(seq)\n        targets.append(label)\n    return np.array(sequences), np.array(targets)\n\n# 데이터 전처리\nX_scaled, y = preprocess_data(data)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 데이터 길이 체크\nif len(X_scaled) < sequence_length:\n    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n\n# 시퀀스 데이터 생성\ny_array = y.values  # pandas Series를 numpy array로 변환\nX_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n\n# 생성된 시퀀스 데이터의 형태 확인\nprint(f\"X_seq shape: {X_seq.shape}\")\nprint(f\"y_seq shape: {y_seq.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 전처리 함수 V2\ndef preprocess_data(data):\n    # 목표 변수 생성\n    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n    \n    # 피처 열만 선택\n    feature_columns = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                       'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                       'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                       'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                       'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                       'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                       'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                       'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                       'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                       'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                       'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n    # 특성과 목표 변수 분리\n    X = data[feature_columns]\n    y = data['target']\n\n    # 무한대 값을 NaN으로 대체\n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    X_imputed = imputer.fit_transform(X)\n\n    # 데이터 정규화\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X_imputed)\n\n    return X_scaled, y\n\n# 시계열 데이터 형태로 변환 함수\ndef create_sequences(data, target, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        label = target[i + sequence_length - 1]\n        sequences.append(seq)\n        targets.append(label)\n    return np.array(sequences), np.array(targets)\n\n# 데이터 전처리\nX_scaled, y = preprocess_data(data)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 데이터 길이 체크\nif len(X_scaled) < sequence_length:\n    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n\n# 시퀀스 데이터 생성\ny_array = y.values  # pandas Series를 numpy array로 변환\nX_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n\n# 생성된 시퀀스 데이터의 형태 확인\nprint(f\"X_seq shape: {X_seq.shape}\")\nprint(f\"y_seq shape: {y_seq.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**테스트 데이터 처리**","metadata":{}},{"cell_type":"code","source":"data_test_tmp = pd.read_csv(\"/kaggle/input/indicator3-test-data/SOL_Data_Test_Indicator3.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#원형\ndata_test = data_test_tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6등분 => 2개월\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4등분 => 3개월\ndef extract_last_fourth(df):\n    num_rows = len(df)\n    fourth_size = num_rows // 4\n    start_index = 3 * fourth_size\n    end_index = num_rows\n    last_fourth = df.iloc[start_index:end_index]\n    return last_fourth\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_fourth(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 => 4개월\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_tmp.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 5개월\ndef extract_last_five_parts(df):\n    num_rows = len(df)\n    part_size = num_rows // 12  # 각 등분의 크기 계산\n    start_index = part_size * 7  # 맨 뒤 7등분의 시작 인덱스\n    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n    return last_five_parts\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_five_parts(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분 => 6개월\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 7개월\ndef extract_last_five_parts(df):\n    num_rows = len(df)\n    part_size = num_rows // 12  # 각 등분의 크기 계산\n    start_index = part_size * 5  # 맨 뒤 5등분의 시작 인덱스\n    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n    return last_five_parts\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_five_parts(data_test_tmp)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 마지막 등분\n# 데이터프레임의 길이 계산\ntotal_length = len(data_test)\n\n# 마지막 1/3 데이터 분리\ndata_test = data_test.iloc[-(total_length // 3):]\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분 한 것의 두번째\n# 데이터프레임의 길이 계산\ntotal_length = len(data_test)\n\n# 3등분으로 나눈 길이 계산\nsplit_length = total_length // 3\n\n# 중간 부분 데이터 분리\ndata_test = data_test.iloc[split_length:split_length*2]\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open_time 열을 datetime 형식으로 변환\nif not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# time 열을 분 단위로 변환\ndata_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0.1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata_test_predict = data_test.drop(columns=['open_time', 'max_return_60min', 'min_return_60min'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외 v2\n# 피처 목록\nfeatures_to_keep = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                    'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                    'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                    'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                    'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                    'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                    'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                    'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                    'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                    'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                    'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n\n# 피처들만 남기기\ndata_test_predict = data_test[features_to_keep]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n\n# 데이터 정규화\nscaler = MinMaxScaler()\ndata_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n\n# 예측 데이터를 시퀀스 형태로 변환 (LSTM용)\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TCN(시계열)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TCN 모델 정의\nclass TCNModel(nn.Module):\n    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_size) -> (batch_size, input_size, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 모델 설정\ninput_size = X_train.shape[2]\nnum_channels = 64\nmodel = TCNModel(input_size, num_channels)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install xgboost\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# XGBoost 모델 학습\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# 피처 중요도 평가\nfeature_importances = model.feature_importances_\nfeature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 피처 중요도를 데이터프레임으로 변환\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# 중요도에 따라 정렬\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n\n# 피처 중요도 표 출력\nfeature_importance_df.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SequentialFeatureSelector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 순차적 전진 선택\nmodel = LogisticRegression(max_iter=10000)\nsfs = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward')\nsfs.fit(X_train, y_train)\n\n# 선택된 피처\nselected_features = sfs.get_support(indices=True)\nselected_feature_names = data.drop(columns=['max_return_60min', 'min_return_60min', 'target']).columns[selected_features]\n\n# 선택된 피처 중요도 (일반적으로 Logistic Regression에서는 coef_ 속성을 사용하여 피처 중요도를 평가할 수 있습니다)\nmodel.fit(sfs.transform(X_train), y_train)\nfeature_importances = model.coef_[0]\n\n# 선택된 피처 중요도를 데이터프레임으로 변환\nfeature_importance_df = pd.DataFrame({\n    'Feature': selected_feature_names,\n    'Importance': feature_importances\n})\n\n# 중요도에 따라 정렬\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n\n# 피처 중요도 표 출력\nprint(feature_importance_df)\n\n# 모델 성능 평가\naccuracy = model.score(sfs.transform(X_test), y_test)\nprint('Model accuracy with selected features:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_TCN_v4_180_11.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 테스트 데이터 텐서로 변환\nX_test_tensor = torch.tensor(X_seq, dtype=torch.float32)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/3/SOL60_SMALLL_INDICATOR3_TCN_v2.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.train()  # 모델을 학습 모드로 설정\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#추가 학습\n# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 추가 학습 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_TCN_v3.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n\n# 테스트 데이터 텐서로 변환\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_test_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/5/SOL60_SMALLL_INDICATOR3_TCN_v4.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequence_length를 사용하여 X_test_seq의 shape를 맞춤\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 예측 수행\nmodel.eval()\nwith torch.no_grad():\n    predictions = torch.sigmoid(model(X_test_tensor)).squeeze().numpy()\n\n# 예측 결과를 이진 분류로 변환 (0 또는 1)\npredictions = (predictions > 0.5).astype(int)\n\n# 예측 결과를 데이터프레임에 추가\ndata_test['prediction_Transformer'] = np.nan  # 예측 결과를 담을 열을 초기화\ndata_test.iloc[sequence_length - 1:sequence_length - 1 + len(predictions), data_test.columns.get_loc('prediction_Transformer')] = predictions\n\n# 결과 확인\nprint(data_test[['open_time', 'prediction_Transformer']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.dropna(subset=['prediction_Transformer'])\n# 결과 출력\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4등분하여 마지막 등분을 추출하는 함수\ndef extract_last_quarter(df):\n    num_rows = len(df)\n    quarter_size = num_rows // 4\n    start_index = 3 * quarter_size\n    end_index = num_rows\n    last_quarter = df.iloc[start_index:end_index]\n    return last_quarter\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_quarter(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    fifth_size = num_rows // 5\n    start_index = 4 * fifth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 5등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 7\n    start_index = 6 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['prediction_Transformer'] == 0)])\n\nprint(f\"max_return_60min이 1 이상인데 prediction이 0인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1 미만인데 prediction이 1인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 조건을 만족하는 데이터 필터링\nfiltered_data = data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 0)]\n\n# min_return_60min 값의 분포를 히스토그램으로 시각화\nplt.figure(figsize=(10, 6))\nplt.hist(filtered_data['min_return_60min'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Distribution of min_return_60min for max_return_60min >= 1.1 and prediction_Transformer == 0')\nplt.xlabel('min_return_60min')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 함수 정의: 각 행에 대해 60행 이내의 high와 low 값을 비교하여 시각을 찾는 함수\ndef find_high_low_times(df, window=60):\n    max_high_times = []\n    min_low_times = []\n\n    for i in range(len(df)):\n        # 현재 행부터 60행 이내의 데이터를 선택\n        subset = df.iloc[i:i+window]\n\n        # 최대 high와 최소 low 값을 찾음\n        max_high_idx = subset['high'].idxmax()\n        min_low_idx = subset['low'].idxmin()\n\n        # 해당 인덱스의 시간을 저장\n        max_high_time = df.at[max_high_idx, 'open_time']\n        min_low_time = df.at[min_low_idx, 'open_time']\n\n        max_high_times.append(max_high_time)\n        min_low_times.append(min_low_time)\n\n    df['max_high_time'] = max_high_times\n    df['min_low_time'] = min_low_times\n\n    return df\n\n# 함수 호출\ndata_test = find_high_low_times(data_test)\ndata_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 조건에 맞는 새로운 열 추가\ndef add_condition_column(df):\n    # 'max_high_time'과 'min_low_time'이 datetime 형식인지 확인하고 변환\n    df['max_high_time'] = pd.to_datetime(df['max_high_time'])\n    df['min_low_time'] = pd.to_datetime(df['min_low_time'])\n    \n    # 조건에 맞는 행 필터링 (max_return_60min >= 1.1 및 prediction_Transformer == 1)\n    filtered_df = df[(df['max_return_60min'] >= 1.1) & (df['prediction_Transformer'] == 1)].copy()\n    \n    # 새로운 열 추가 및 초기화\n    filtered_df['condition'] = 0\n    \n    # 조건을 만족하는 경우 condition 값을 설정\n    condition_indices = filtered_df.index[filtered_df['max_high_time'] < filtered_df['min_low_time']]\n    \n    filtered_df.loc[condition_indices, 'condition'] = 1\n    \n    return filtered_df\n\n# 데이터 타입 변환\ndata_test['max_high_time'] = pd.to_datetime(data_test['max_high_time'])\ndata_test['min_low_time'] = pd.to_datetime(data_test['min_low_time'])\n\n# 조건 열 추가\nfiltered_data_test = add_condition_column(data_test)\n\n# 결과 확인\nfiltered_data_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"condition_1_data = filtered_data_test[filtered_data_test['condition'] == 0]\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_high_time과 min_low_time의 차이를 분으로 계산하여 새로운 열 추가\ncondition_1_data['time_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['min_low_time']).dt.total_seconds() / 60\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#min_low_time과 open_time의 차이를 분으로 계산하여 새로운 열 추가\ncondition_1_data['min_low_open_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['open_time']).dt.total_seconds() / 60\ncondition_1_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\ntime_difference_max = condition_1_data['min_low_open_difference_minutes'].max()\ntime_difference_min = condition_1_data['min_low_open_difference_minutes'].min()\ntime_difference_mode = condition_1_data['min_low_open_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\ntime_difference_mean = condition_1_data['min_low_open_difference_minutes'].mean()\n\n# 결과 출력\ntime_difference_stats = {\n    'max': time_difference_max,\n    'min': time_difference_min,\n    'mode': time_difference_mode, #최빈\n    'mean': time_difference_mean # 평균\n}\n\ntime_difference_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_return_60min이 -1 이하인 행들 필터링\nnegative_return_data = condition_1_data[condition_1_data['min_return_60min'] <= -1]\n\n# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\ntime_difference_max = negative_return_data['time_difference_minutes'].max()\ntime_difference_min = negative_return_data['time_difference_minutes'].min()\ntime_difference_mode = negative_return_data['time_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\ntime_difference_mean = negative_return_data['time_difference_minutes'].mean()\n\n# 결과 출력\ntime_difference_stats = {\n    'max': time_difference_max,\n    'min': time_difference_min,\n    'mode': time_difference_mode,  # 최빈값\n    'mean': time_difference_mean   # 평균값\n}\n\ntime_difference_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습 데이터와 검증 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터를 텐서로 변환\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 데이터 로더 생성\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 모델 설정\ninput_dim = X_train.shape[2]\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 학습률 감소 스케줄러 설정\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# 조기 종료 설정\npatience = 5\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 모델 저장 경로\nbest_model_path = \"best_transformer_model.pth\"\n\n# 모델 학습\nnum_epochs = 20  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Duration: {epoch_duration:.2f}s')\n\n    # 최적의 모델 저장 및 조기 종료 조건 체크\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Best model saved with val loss: {best_val_loss:.4f}\")\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n    # 학습률 감소\n    scheduler.step()\n\n# 학습 및 검증 손실 시각화\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 최적의 모델 로드\nmodel.load_state_dict(torch.load(best_model_path))\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install ipywidgets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 입력 차원 확인 및 설정\ninput_dim = X_train.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/6/SOL60_SMALLL_INDICATOR3_Transformer_v3.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.train()  # 추가 학습을 위해 학습 모드로 전환\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#추가학습\n\n# 손실 함수 및 옵티마이저 설정\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 조기 종료 설정\npatience = 5\nbest_loss = float('inf')\npatience_counter = 0\n\n# 학습 및 검증 손실을 저장할 리스트\ntrain_losses = []\nval_losses = []\n\n# 추가 학습\nnum_epochs = 100  # 최대 에포크 수\nfor epoch in range(num_epochs):\n    # 학습 단계\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # 검증 단계\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n    # 조기 종료 조건 체크\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= patience:\n        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n        break\n\n# 학습 및 검증 손실 시각화\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n# 이진 분류 결과를 위한 평가 지표 계산\ny_pred = np.array(y_pred) > 0.5\ny_true = y_test_tensor.numpy()\n\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n# 모델 정의 (로드할 때 필요)\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output\n\n# 입력 차원 확인 및 설정\ninput_dim = X_test_seq.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/5/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequence_length를 사용하여 X_test_seq의 shape를 맞춤\nsequence_length = X_test_seq.shape[1]\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).view(-1, sequence_length, input_dim)\n\n# 예측 수행\nwith torch.no_grad():\n    predictions = torch.sigmoid(model(X_test_tensor)).squeeze().numpy()\n\n# 예측 결과를 이진 분류로 변환 (0 또는 1)\npredictions = (predictions > 0.5).astype(int)\n\n# 예측 결과를 데이터프레임에 추가\ndata_test['prediction_Transformer'] = np.nan  # 예측 결과를 담을 열을 초기화\ndata_test.iloc[sequence_length - 1:sequence_length - 1 + len(predictions), data_test.columns.get_loc('prediction_Transformer')] = predictions\n\n# 결과 확인\nprint(data_test[['open_time', 'prediction_Transformer']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test_non_nan = data_test.dropna(subset=['prediction_Transformer'])\n\n# max_return_60min 기준으로 내림차순 정렬\ndata_test_sorted = data_test_non_nan.sort_values(by='max_return_60min', ascending=False)\n\n# 결과 출력\ndata_test_sorted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2등분\ndef extract_last_half(df):\n    num_rows = len(df)\n    half_size = num_rows // 2\n    start_index = half_size\n    end_index = num_rows\n    last_half = df.iloc[start_index:end_index]\n    return last_half\n\n# 데이터프레임 2등분하여 마지막 등분 추출\ndata_test = extract_last_half(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3등분\ndef extract_last_third(df):\n    num_rows = len(df)\n    third_size = num_rows // 3\n    start_index = 2 * third_size\n    end_index = num_rows\n    last_third = df.iloc[start_index:end_index]\n    return last_third\n\n# 데이터프레임 3등분하여 마지막 등분 추출\ndata_test = extract_last_third(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4등분하여 마지막 등분을 추출하는 함수\ndef extract_last_quarter(df):\n    num_rows = len(df)\n    quarter_size = num_rows // 4\n    start_index = 3 * quarter_size\n    end_index = num_rows\n    last_quarter = df.iloc[start_index:end_index]\n    return last_quarter\n\n# 데이터프레임 4등분하여 마지막 등분 추출\ndata_test = extract_last_quarter(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    fifth_size = num_rows // 5\n    start_index = 4 * fifth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 5등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 6\n    start_index = 5 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7등분하여 마지막 등분을 추출하는 함수\ndef extract_last_sixth(df):\n    num_rows = len(df)\n    sixth_size = num_rows // 7\n    start_index = 6 * sixth_size\n    end_index = num_rows\n    last_sixth = df.iloc[start_index:end_index]\n    return last_sixth\n\n# 데이터프레임 6등분하여 마지막 등분 추출\ndata_test = extract_last_sixth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12등분 => 1개월\ndef extract_last_twelfth(df):\n    num_rows = len(df)\n    twelfth_size = num_rows // 12\n    start_index = 11 * twelfth_size\n    end_index = num_rows\n    last_twelfth = df.iloc[start_index:end_index]\n    return last_twelfth\n\n# 데이터프레임 12등분하여 마지막 등분 추출\ndata_test = extract_last_twelfth(data_test)\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['prediction_Transformer'] == 0)])\n\nprint(f\"max_return_60min이 1 이상인데 prediction이 0인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1 미만인데 prediction이 1인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TransformerEncoder 모델 정의\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])\n        return output\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        running_loss += loss.item()\n    avg_train_loss = running_loss / len(train_loader)\n    return avg_train_loss\n\ndef validate_model(model, test_loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            val_loss += loss.item()\n    avg_val_loss = val_loss / len(test_loader)\n    return avg_val_loss\n\ndef _mp_fn(rank, flags):\n    # 데이터 로더 생성\n    train_loader = pl.MpDeviceLoader(train_dataset, xm.xla_device())\n    test_loader = pl.MpDeviceLoader(test_dataset, xm.xla_device())\n    \n    input_dim = X_train.shape[2]\n    nhead = 2\n    num_layers = 2\n    dim_feedforward = 64\n    output_dim = 1\n\n    model = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim).to(xm.xla_device())\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n    patience = 5\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    train_losses = []\n    val_losses = []\n\n    num_epochs = 50\n    print_freq = 5\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        avg_train_loss = train_model(model, train_loader, criterion, optimizer, xm.xla_device())\n        train_losses.append(avg_train_loss)\n        \n        avg_val_loss = validate_model(model, test_loader, criterion, xm.xla_device())\n        val_losses.append(avg_val_loss)\n\n        end_time = time.time()\n        epoch_duration = end_time - start_time\n\n        if (epoch + 1) % print_freq == 0 or epoch == num_epochs - 1:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Duration: {epoch_duration:.2f}s')\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            xm.save(model.state_dict(), 'best_transformer_model.pth')\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n            break\n\n        scheduler.step()\n\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('training_validation_loss.png')  # 학습 곡선 저장\n    plt.show()\n\n    # 모델을 반환하기 위해 로드하고 저장 경로 설정\n    model.load_state_dict(torch.load('best_transformer_model.pth'))\n    model.to(xm.xla_device())\n\n    model.eval()\n    with torch.no_grad():\n        y_true = []\n        y_pred = []\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(xm.xla_device()), y_batch.to(xm.xla_device())\n            output = model(X_batch)\n            y_true.extend(y_batch.tolist())\n            y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n    y_pred = np.array(y_pred) > 0.5\n    y_true = y_test_tensor.numpy()\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n    print(f'F1 Score: {f1:.4f}')\n\n    # 혼동 행렬 시각화\n    plt.figure(figsize=(6, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')  # 혼동 행렬 저장\n    plt.show()\n\n    # 모델 저장 경로\n    model_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v4.pth'\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n\n    return model  # 모델 반환\n\nFLAGS = {}\nmodel = xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2nd Account\n# 모델 저장 경로\nmodel_path = '/kaggle/working/SOL60_SMALLL_INDICATOR3_Transformer_v4.pth'\n\n# 모델 상태 저장\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"시간대 분석","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n\n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n\n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_imputed = imputer.fit_transform(data)\n\n# numpy 배열을 다시 DataFrame으로 변환\ndata = pd.DataFrame(data_imputed, columns=data.columns)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nresults = results.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(results['time'], results['percentage_above_1_1'], marker='o', linestyle='-')\nplt.title('Percentage of max_return_60min >= 1.1 over Time')\nplt.xlabel('Time')\nplt.ylabel('Percentage of max_return_60min >= 1.1')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 값을 시-분 형식으로 변환하는 함수\ndef convert_minutes_to_time(minutes):\n    hours = minutes // 60\n    mins = minutes % 60\n    return f\"{int(hours):02}:{int(mins):02}\"\n\n# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# time 값을 시-분 형식으로 변환\nresults = results.reset_index()\nresults['time'] = results['time'].apply(convert_minutes_to_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time 열을 기준으로 그룹화하여 필요한 계산 수행\nresults = data.groupby('time').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nresults = results.reset_index()\n\n# 슬라이딩 윈도우의 크기 설정 (예: 60분)\nwindow_size = 60\n\n# 슬라이딩 윈도우 평균 계산\nresults['rolling_mean'] = results['percentage_above_1_1'].rolling(window=window_size // 10, min_periods=1).mean()\n\n# 임계값 설정\nthreshold = 33  # 여기서 임계값을 설정하세요\n\n# 임계값 이상의 구간 필터링\nhigh_prob_zones = results[results['rolling_mean'] >= threshold]\n\n# 연속된 구간의 시작과 끝을 식별\nhigh_prob_zones['group'] = (high_prob_zones['time'].diff() > 10).cumsum()\n\n# 각 그룹의 시작과 끝 시간 구하기\ngrouped_zones = high_prob_zones.groupby('group').agg(\n    start_time=('time', 'first'),\n    end_time=('time', 'last')\n).reset_index()\n\n# 시간축 이동\nshift_point = 200\nbefore_shift = results[results['time'] < shift_point].copy()\nafter_shift = results[results['time'] >= shift_point].copy()\n\nbefore_shift['shifted_time'] = before_shift['time'] + 1440\nafter_shift['shifted_time'] = after_shift['time']\n\nresults_shifted = pd.concat([after_shift, before_shift]).reset_index(drop=True)\n\n# 시각화\nplt.figure(figsize=(12, 6))\nplt.plot(results_shifted['shifted_time'], results_shifted['percentage_above_1_1'], marker='o', linestyle='-', label='Percentage over Time')\nplt.plot(results_shifted['shifted_time'], results_shifted['rolling_mean'], linestyle='-', color='orange', label='Rolling Mean')\n\n# 임계값 선 추가\nplt.axhline(y=threshold, color='purple', linestyle='-', label=f'Threshold {threshold}%')\n\n# 영역 색칠\nplt.fill_between(results_shifted['shifted_time'], threshold, results_shifted['percentage_above_1_1'], where=(results_shifted['percentage_above_1_1'] >= threshold), interpolate=True, color='purple', alpha=0.3)\n\n# 높은 확률 구간에서 시작과 끝 구간만 표시\nfor _, row in grouped_zones.iterrows():\n    if row['start_time'] < shift_point:\n        shifted_start_time = row['start_time'] + 1440\n    else:\n        shifted_start_time = row['start_time']\n    if row['end_time'] < shift_point:\n        shifted_end_time = row['end_time'] + 1440\n    else:\n        shifted_end_time = row['end_time']\n    plt.axvline(x=shifted_start_time, color='red', linestyle='--', label='Start Time' if row['start_time'] == grouped_zones['start_time'].iloc[0] else \"\")\n    plt.axvline(x=shifted_end_time, color='green', linestyle='--', label='End Time' if row['end_time'] == grouped_zones['end_time'].iloc[0] else \"\")\n\n# 0 지점을 나타내는 세로선 추가\nplt.axvline(x=shift_point, color='blue', linestyle=':', label='Midnight Transition')\n\nplt.title('Percentage of max_return_60min >= 1.1 over Time')\nplt.xlabel('Time (minutes)')\nplt.ylabel('Percentage of max_return_60min >= 1.1')\nplt.xticks(rotation=45)\nplt.xlim(shift_point, 1440 + shift_point)\nplt.xticks(np.arange(shift_point, 1440 + shift_point + 1, 100), labels=[str(x % 1440) for x in np.arange(shift_point, 1440 + shift_point + 1, 100)])\nplt.xticks(list(plt.xticks()[0]) + [0], labels=list(plt.xticks()[1]) + ['0'])  # 0을 x축에 추가\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 분을 시-분 형식으로 변환하는 함수\ndef convert_minutes_to_time(minutes):\n    hours = minutes // 60\n    mins = minutes % 60\n    return f\"{int(hours):02}:{int(mins):02}\"\n\n# 한국 시간으로 변환하는 함수\ndef convert_to_kst(utc_minutes):\n    kst_minutes = (utc_minutes + 540) % 1440  # 한국 시간은 UTC+9 (540분)\n    return convert_minutes_to_time(kst_minutes)\n\n# 높은 확률 구간 시작과 끝 값 출력\nprint(f\"High probability zones above {threshold}% (start_time, end_time):\")\nfor _, row in grouped_zones.iterrows():\n    start_time_utc = convert_minutes_to_time(row['start_time'])\n    end_time_utc = convert_minutes_to_time(row['end_time'])\n    start_time_kst = convert_to_kst(row['start_time'])\n    end_time_kst = convert_to_kst(row['end_time'])\n    print(f\"Start: {start_time_utc} (KST: {start_time_kst}), End: {end_time_utc} (KST: {end_time_kst})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"요일분석","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n        \n    # 요일 추출하여 data_day 변수에 저장\n    data_day = data['open_time'].dt.day_name()\n    \n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute    \n    \n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    # 무한대 값을 NaN으로 대체\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    data_imputed = imputer.fit_transform(data)\n\n    # numpy 배열을 다시 DataFrame으로 변환\n    data = pd.DataFrame(data_imputed, columns=data.columns)\n    \n    data['day'] = data_day\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 그룹화하여 집계 계산\nresults = data.groupby('day').agg(\n    total_rows=('max_return_60min', 'size'),\n    count_above_1_1=('max_return_60min', lambda x: (x >= 1.1).sum())\n)\n\n# 비율 계산\nresults['percentage_above_1_1'] = (results['count_above_1_1'] / results['total_rows']) * 100\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 요일 순서 정렬\nordered_days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nresults = results.reindex(ordered_days)\n\n# 결과 출력\nprint(results)\n\n# 시각화\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# 막대 그래프 생성\nresults['percentage_above_1_1'].plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n\n# 그래프 제목 및 축 레이블 설정\nax.set_title('Percentage of max_return_60min >= 1.1 by Day of Week')\nax.set_xlabel('Day of Week')\nax.set_ylabel('Percentage (%)')\nax.set_xticklabels(ordered_days, rotation=45, ha='right')\n\n# 값 레이블 추가\nfor i in ax.containers:\n    ax.bar_label(i, fmt='%.1f%%')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"변동에 대한 최대 하락 구간의 분포","metadata":{}},{"cell_type":"code","source":"pip install mplfinance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport mplfinance as mpf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#메인 데이터\ndata = pd.read_csv(\"/kaggle/input/indicator3-full/SOL60_INDICATOR3\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n        \n    # 요일 추출하여 data_day 변수에 저장\n    data_day = data['open_time'].dt.day_name()\n    \n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute    \n    \n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time', 'Unnamed: 0', 'Unnamed: 0.1'])\n    \n    # 무한대 값을 NaN으로 대체\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    data_imputed = imputer.fit_transform(data)\n\n    # numpy 배열을 다시 DataFrame으로 변환\n    data = pd.DataFrame(data_imputed, columns=data.columns)\n    \n    data['day'] = data_day\n    \n    return data\n# 시간 관련 열 변환\ndata = convert_time_features(data)\n\n# 시간 변환 함수\ndef convert_time_to_hhmm(df, time_col='time'):\n    # 분 단위를 시-분 형식으로 변환\n    df['hhmm_time'] = pd.to_datetime(df[time_col], unit='m').dt.strftime('%H:%M')\n    return df\n\n# 시간 변환 함수 적용\ndata = convert_time_to_hhmm(data)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min이 1.1 이상인 데이터 필터링\nfiltered_data = data[data['max_return_60min'] >= 1.5]\n\n# hhmm_time을 datetime 형식으로 변환\nfiltered_data['hhmm_time'] = pd.to_datetime(filtered_data['hhmm_time'], format='%H:%M')\n\n# hhmm_time 열을 기준으로 그룹화하여 min_return_60min의 분포를 캔들 스틱으로 표시\ncandlestick_data = filtered_data.groupby(filtered_data['hhmm_time']).agg(\n    Open=('min_return_60min', 'first'),\n    High=('min_return_60min', 'max'),\n    Low=('min_return_60min', 'min'),\n    Close=('min_return_60min', 'last')\n)\n\n# 인덱스를 DatetimeIndex로 설정\ncandlestick_data.index = pd.to_datetime(candlestick_data.index)\n\n# 캔들 스틱 차트 시각화\nmpf.plot(candlestick_data, type='candle', style='charles', title='Candlestick chart of min_return_60min', ylabel='min_return_60min')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min과 min_return_60min 열을 마지막 부분으로 이동\ncolumns = list(data.columns)\ncolumns.remove('max_return_60min')\ncolumns.remove('min_return_60min')\ncolumns.extend(['max_return_60min', 'min_return_60min'])\ndata = data[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_return_60min이 1.1 이상이면서 min_return_60min이 1 이하인 데이터 필터링\nfiltered_data2 = data[(data['max_return_60min'] >= 1.5) & (data['min_return_60min'] <= -10)]\n\n# 결과 출력\nfiltered_data2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###########################Trailing Test######################","metadata":{}},{"cell_type":"code","source":"#테스트 데이터 호출\ndata_test_tmp = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL_Data_Test_Indicator3.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#테스트 데이터 호출\ndata_test_tmp = pd.read_csv(\"/kaggle/input/data-set-plus-test-24-08-07/SOL_Data_Test_Indicator3_180.csv\")\n\n# Display the first few rows of the dataset to ensure it is loaded correctly\ndata_test_tmp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test_tmp\ndata_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = data_test.drop(columns=['Unnamed: 0'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open_time 열을 datetime 형식으로 변환\nif not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# time 열을 분 단위로 변환\ndata_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외\ndata_test_predict = data_test.drop(columns=['open_time', 'max_return_60min', 'min_return_60min'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용하지 않을 열 제외 v2\n# 피처 목록\nfeatures_to_keep = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10', \n                    'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30', \n                    'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200', \n                    'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20', \n                    'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200', \n                    'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20', \n                    'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40', \n                    'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20', \n                    'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close', \n                    'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50', \n                    'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n\n# 피처들만 남기기\ndata_test_predict = data_test[features_to_keep]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 무한대 값을 NaN으로 대체\ndata_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# NaN 값을 평균으로 대체\nimputer = SimpleImputer(strategy='mean')\ndata_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n\n# 데이터 정규화\nscaler = MinMaxScaler()\ndata_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformer model 호출\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# 모델 정의 (로드할 때 필요)\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n        super(TransformerEncoderModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, input_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, src):\n        src = self.embedding(src)\n        output = self.transformer_encoder(src)\n        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformer model 호출2\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측 데이터를 시퀀스 형태로 변환\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n\n# 입력 차원 확인 및 설정\ninput_dim = X_test_seq.shape[2]\nprint(f\"Input dimension: {input_dim}\")\n\n# 모델 설정 (로드할 때 필요)\nnhead = 2\nnum_layers = 2\ndim_feedforward = 64\noutput_dim = 1\n\nmodel = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/6/SOL60_SMALLL_INDICATOR3_Transformer_v3.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TCN 모델 호출\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#모델 로드\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_channels, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_channels) -> (batch_size, input_channels, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TCN 모델 호출2\n# 시퀀스 길이 설정\nsequence_length = 60\n\n# 예측 데이터를 시퀀스 형태로 변환\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# 예측용 시퀀스 데이터 생성\nX_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n\n# 입력 차원 확인 및 설정\ninput_channels = X_test_seq.shape[2]\nprint(f\"Input channels: {input_channels}\")\n\n# 모델 설정\nnum_channels = 64\nmodel = TCNModel(input_channels, num_channels)\n\n# 모델 로드\nmodel_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/9/SOL60_SMALLL_INDICATOR3_TCN_v4_180_11.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nprint(f\"Model loaded from {model_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 슬라이딩 윈도우로 데이터 범위 추출\nnum_rows = data_test_predict_scaled.shape[0]\n\n# 시퀀스 데이터를 텐서로 변환\nX_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\nprint(f\"Tensor Transform Complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델을 GPU로 이동 (가능한 경우)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nX_test_tensor = X_test_tensor.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prediction 결과 저장\nresults = []\n\n#window_size = 216000\n#window_size = 259200\nwindow_size = 302400\n\nbatch_size = 63  # 배치 크기 설정\n\nfor end in tqdm(range(num_rows, window_size - 1, -batch_size)):\n    start = max(end - batch_size + 1, 0)\n    \n    # 해당 범위에 대한 시퀀스 텐서 추출\n    X_test_tensor_tmp = X_test_tensor[start:end]\n    \n    # 예측 수행\n    model.eval()\n    with torch.no_grad():\n        predictions = torch.sigmoid(model(X_test_tensor_tmp)).squeeze().cpu().numpy()\n\n    # 예측 결과를 이진 분류로 변환 (0 또는 1)\n    predictions = (predictions > 0.5).astype(int)\n    \n    # 예측 결과의 마지막 값을 추가\n    if len(predictions.shape) > 0:\n        results.append(predictions[-1])\n    else:\n        results.append(predictions)\n\nresults = results[::-1]  # 원래 순서대로 변경","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 결과를 원본 데이터프레임에 추가\ndata_test['Predictions'] = np.nan\ndata_test.loc[data_test.index[-len(results):], 'Predictions'] = results\ndata_test = data_test.dropna(subset=['Predictions'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\ncount_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['Predictions'] == 1)])\n\n# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\ncount_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['Predictions'] == 0)])\n    \n#\nprint(f\"max_return_60min이 1.1 이상인데 prediction이 1인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\nprint(f\"max_return_60min이 1.1 미만인데 prediction이 0인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 나타내는 새로운 열 추가 (30분 단위)\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# 시간대별로 데이터 그룹화 (30분 단위)\ngrouped = data_test.groupby('half_hour')\n\n# 각 시간대별로 조건을 만족하는 데이터의 개수를 계산하는 함수\ndef calculate_counts(group):\n    count_max_return_ge_1_prediction_0 = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    count_max_return_lt_1_prediction_1 = len(group[(group['max_return_60min'] < 1.1) & (group['Predictions'] == 0)])\n    \n    total_count = len(group)\n    \n    if total_count == 0:\n        return pd.Series([0, 0, 0, 0, 0])\n    \n    rate_max_return_ge_1_prediction_0 = (count_max_return_ge_1_prediction_0 / total_count) * 100\n    rate_max_return_lt_1_prediction_1 = (count_max_return_lt_1_prediction_1 / total_count) * 100\n    \n    total_rate = rate_max_return_ge_1_prediction_0 + rate_max_return_lt_1_prediction_1\n    \n    return pd.Series([count_max_return_ge_1_prediction_0, count_max_return_lt_1_prediction_1, rate_max_return_ge_1_prediction_0, rate_max_return_lt_1_prediction_1, total_rate])\n\n# 각 그룹에 함수 적용\nresults = grouped.apply(calculate_counts)\nresults.columns = ['Count_GE_1_Pred_0', 'Count_LT_1_Pred_1', 'Rate_GE_1_Pred_0', 'Rate_LT_1_Pred_1', 'Total_Rate']\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 UTC 및 KST로 표시\nresults['half_hour_utc'] = pd.to_datetime(results.index.astype(str), format='%H:%M:%S').time\nresults['half_hour_kst'] = (pd.to_datetime(results.index.astype(str), format='%H:%M:%S') + pd.Timedelta(hours=9)).time\nresults['half_hour_label'] = results.index.astype(str) + ' (' + results['half_hour_kst'].astype(str) + ' KST)'\n\n# x축을 시간으로, y축을 Total_Rate으로 하는 차트 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(results['half_hour_label'], results['Total_Rate'], marker='o')\nplt.xlabel('Half Hour of the Day (UTC)')\nplt.ylabel('Total Rate (%)')\nplt.title('Total Rate of Conditions Met by Half Hour of the Day (UTC with KST)')\nplt.grid(True)\nplt.xticks(rotation=45, ha='right')  # 레이블을 오른쪽으로 회전하여 수평으로 정렬\n\n# y축 90 라인 진하게 표시\nplt.axhline(90, color='red', linewidth=1.5, linestyle='--')  # y=90 라인을 진하게\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 나타내는 새로운 열 추가 (30분 단위)\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# 시간대별로 데이터 그룹화 (30분 단위)\ngrouped = data_test.groupby('half_hour')\n\n# 각 시간대별로 max_return_60min이 1.1 이상이고 Predictions가 1인 값의 비율을 계산하는 함수\ndef calculate_ratio(group):\n    count_condition_met = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    total_count = len(group)\n    \n    if total_count == 0:\n        return 0\n    \n    return (count_condition_met / total_count) * 100\n\n# 각 그룹에 함수 적용\nresults = grouped.apply(calculate_ratio)\nresults = results.reset_index()\nresults.columns = ['half_hour', 'Condition_Met_Ratio']\n\n# 결과 출력\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시간대를 UTC 및 KST로 표시\nresults['half_hour'] = pd.to_datetime(results['half_hour'].astype(str), format='%H:%M:%S').dt.strftime('%H:%M')\nresults['half_hour_kst'] = (pd.to_datetime(results['half_hour'], format='%H:%M') + pd.Timedelta(hours=9)).dt.strftime('%H:%M')\nresults['half_hour_label'] = results['half_hour'] + ' (' + results['half_hour_kst'] + ' KST)'\n\n# x축을 시간으로, y축을 Condition_Met_Ratio로 하는 차트 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(results['half_hour_label'], results['Condition_Met_Ratio'], marker='o')\nplt.xlabel('Half Hour of the Day (UTC)')\nplt.ylabel('Condition Met Ratio (%)')\nplt.title('Ratio of max_return_60min >= 1.1 and Predictions == 1 by Half Hour (UTC with KST)')\nplt.grid(True)\nplt.xticks(rotation=45, ha='right')  # 레이블을 오른쪽으로 회전하여 수평으로 정렬\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'open_time' to datetime if it's not already\ndata_test['open_time'] = pd.to_datetime(data_test['open_time'])\n\n# Add a new column representing half-hour intervals\ndata_test['half_hour'] = data_test['open_time'].dt.floor('30T').dt.time\n\n# Group by the half-hour intervals\ngrouped = data_test.groupby('half_hour')\n\n# Define the function to calculate the counts and rates\ndef calculate_counts(group):\n    count_max_return_ge_1_prediction_0 = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    count_max_return_lt_1_prediction_1 = len(group[(group['max_return_60min'] < 1.1) & (group['Predictions'] == 0)])\n    \n    total_count = len(group)\n    \n    if total_count == 0:\n        return pd.Series([0, 0, 0, 0, 0])\n    \n    rate_max_return_ge_1_prediction_0 = (count_max_return_ge_1_prediction_0 / total_count) * 100\n    rate_max_return_lt_1_prediction_1 = (count_max_return_lt_1_prediction_1 / total_count) * 100\n    \n    total_rate = rate_max_return_ge_1_prediction_0 + rate_max_return_lt_1_prediction_1\n    \n    return pd.Series([count_max_return_ge_1_prediction_0, count_max_return_lt_1_prediction_1, rate_max_return_ge_1_prediction_0, rate_max_return_lt_1_prediction_1, total_rate])\n\n# Apply the function to the grouped data\nresults = grouped.apply(calculate_counts)\nresults.columns = ['Count_GE_1_Pred_0', 'Count_LT_1_Pred_1', 'Rate_GE_1_Pred_0', 'Rate_LT_1_Pred_1', 'Total_Rate']\n\n# Function to calculate the ratio for condition met\ndef calculate_ratio(group):\n    count_condition_met = len(group[(group['max_return_60min'] >= 1.1) & (group['Predictions'] == 1)])\n    total_count = len(group)\n    \n    if total_count == 0:\n        return 0\n    \n    return (count_condition_met / total_count) * 100\n\n# Apply the function to the grouped data\nratio_results = grouped.apply(calculate_ratio)\nratio_results = ratio_results.reset_index()\nratio_results.columns = ['half_hour', 'Condition_Met_Ratio']\n\n# Combine the results into a single DataFrame\ncombined_results = results.reset_index()\ncombined_results['Condition_Met_Ratio'] = ratio_results['Condition_Met_Ratio']\n\n# 시간대를 UTC 및 KST로 표시\ncombined_results['half_hour_utc'] = pd.to_datetime(combined_results['half_hour'].astype(str), format='%H:%M:%S').dt.strftime('%H:%M')\ncombined_results['half_hour_kst'] = (pd.to_datetime(combined_results['half_hour_utc'], format='%H:%M') + pd.Timedelta(hours=9)).dt.strftime('%H:%M')\ncombined_results['half_hour_label'] = combined_results['half_hour_utc'] + ' (' + combined_results['half_hour_kst'] + ' KST)'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the combined results\nfig, ax1 = plt.subplots(figsize=(12, 6))\n\nax1.plot(combined_results['half_hour_label'], combined_results['Total_Rate'], marker='o', color='b', label='Total Rate')\nax1.set_xlabel('Half Hour of the Day (UTC)')\nax1.set_ylabel('Total Rate (%)', color='b')\nax1.tick_params(axis='y', labelcolor='b')\nax1.grid(True)\nplt.xticks(rotation=45, ha='right')\n\nax2 = ax1.twinx()\nax2.plot(combined_results['half_hour_label'], combined_results['Condition_Met_Ratio'], marker='o', color='g', label='Condition Met Ratio')\nax2.set_ylabel('Condition Met Ratio (%)', color='g')\nax2.tick_params(axis='y', labelcolor='g')\n\nfig.tight_layout()\nplt.title('Comparison of Total Rate and Condition Met Ratio by Half Hour (UTC with KST)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Auto_Modeling_Test","metadata":{}},{"cell_type":"code","source":"pip install joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tdqm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nfrom tqdm import tqdm\n\n# TensorFlow 및 TPU 설정\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.impute import SimpleImputer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:38:06.831535Z","iopub.execute_input":"2024-08-10T12:38:06.831863Z","iopub.status.idle":"2024-08-10T12:38:46.862737Z","shell.execute_reply.started":"2024-08-10T12:38:06.831835Z","shell.execute_reply":"2024-08-10T12:38:46.861962Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1723293495.520815      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0810 12:38:15.529120437      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0810 12:38:15.529135659      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0810 12:38:15.529139044      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0810 12:38:15.529141504      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0810 12:38:15.529143837      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0810 12:38:15.529146264      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0810 12:38:15.529148630      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0810 12:38:15.529150914      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0810 12:38:15.529153253      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0810 12:38:15.529155451      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0810 12:38:15.529157667      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0810 12:38:15.529159909      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0810 12:38:15.529169007      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0810 12:38:15.529171303      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0810 12:38:15.529173485      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0810 12:38:15.529175660      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0810 12:38:15.529178006      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0810 12:38:15.529180237      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0810 12:38:15.529182437      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0810 12:38:15.529184653      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0810 12:38:15.529186882      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0810 12:38:15.529189062      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0810 12:38:15.529191415      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0810 12:38:15.529193773      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0810 12:38:15.529196124      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0810 12:38:15.529198455      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0810 12:38:15.529200776      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0810 12:38:15.529203044      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0810 12:38:15.529205379      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0810 12:38:15.529208662      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0810 12:38:15.529210960      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0810 12:38:15.529213339      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0810 12:38:15.529215730      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0810 12:38:15.529217949      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0810 12:38:15.529220095      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0810 12:38:15.529222276      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0810 12:38:15.529224369      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0810 12:38:15.529226539      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0810 12:38:15.529228753      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0810 12:38:15.529230986      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0810 12:38:15.529233125      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0810 12:38:15.529235264      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0810 12:38:15.529237419      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0810 12:38:15.529239654      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0810 12:38:15.529241951      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0810 12:38:15.529403637      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0810 12:38:15.529414912      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD0810 12:38:15.539924988      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0810 12:38:15.539957065      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0810 12:38:15.539966299      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0810 12:38:15.539969313      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0810 12:38:15.539972444      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0810 12:38:15.539975212      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0810 12:38:15.540023576      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0810 12:38:15.540054266      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0810 12:38:15.540093358      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0810 12:38:15.540120511      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0810 12:38:15.540129457      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0810 12:38:15.540132696      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0810 12:38:15.540137265      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0810 12:38:15.540140538      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0810 12:38:15.540143862      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0810 12:38:15.540154603      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0810 12:38:15.540191318      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0810 12:38:15.541987534      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0810 12:38:15.543248794      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0810 12:38:15.565254671     104 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0810 12:38:15.565324625     104 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0810 12:38:15.570747121     101 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-08-10T12:38:15.57073004+00:00\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"#데이터 호출 시리즈\nData_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_1m_Micro_Indicator3.csv', \n             '/kaggle/input/data-set-24-08-09/SOL_Data_3m_Indicator3.csv', \n             '/kaggle/input/data-set-24-08-09/SOL_Data_5m_Indicator3.csv',\n             '/kaggle/input/data-set-24-08-09/SOL_Data_15m_Indicator3.csv',\n             '/kaggle/input/data-set-24-08-09/SOL_Data_30m_Indicator3.csv']\n\nData_save = ['/kaggle/working/SOL_Data_1m_Micro_INDICATOR3_TCN_v4.pth', \n             '/kaggle/working/SOL_Data_3m_Micro_INDICATOR3_TCN_v4.pth', \n             '/kaggle/working/SOL_Data_5m_Micro_INDICATOR3_TCN_v4.pth',\n             '/kaggle/working/SOL_Data_15m_Micro_INDICATOR3_TCN_v4.pth',\n             '/kaggle/working/SOL_Data_30m_Micro_INDICATOR3_TCN_v4.pth']\n\nData_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_1m_Indicator3.csv', \n                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_3m_Indicator3.csv', \n                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_5m_Indicator3.csv',\n                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_15m_Indicator3.csv',\n                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_30m_Indicator3.csv']\n\nTarget_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n\nmention_List = [\"1m\", \"3m\", \"5m\", \"15m\", \"30m\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#데이터 호출 시리즈\nData_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_1m_Micro_Indicator3.csv']\n\nData_save = ['/kaggle/working/SOL_Data_1m_INDICATOR3_TCN_v4.pth']\n\nData_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_1m_Indicator3.csv']\n\nTarget_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n\nmention_List = [\"1m\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#데이터 호출 시리즈\nData_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_3m_Indicator3.csv']\n\nData_save = ['/kaggle/working/SOL_Data_3m_INDICATOR3_TCN_v4.pth']\n\nData_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_3m_Indicator3.csv']\n\nTarget_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n\nmention_List = [\"3m\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:38:46.864134Z","iopub.execute_input":"2024-08-10T12:38:46.865037Z","iopub.status.idle":"2024-08-10T12:38:46.869149Z","shell.execute_reply.started":"2024-08-10T12:38:46.865002Z","shell.execute_reply":"2024-08-10T12:38:46.868388Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#함수 시리즈\n\ndef drop_unnamed_column(df):\n    # 'Unnamed: 0' 열이 존재하는지 확인\n    if 'Unnamed: 0' in df.columns:\n        # 존재한다면 해당 열 삭제\n        df = df.drop(columns=['Unnamed: 0'])\n    return df\n\n# 시간 관련 열 변환 함수\ndef convert_time_features(data):\n    # open_time 열이 datetime 형식이 아닌 경우 변환\n    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n        data['open_time'] = pd.to_datetime(data['open_time'])\n\n    # time 열을 분 단위로 변환\n    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n\n    # 사용하지 않을 열 제외\n    data = data.drop(columns=['open_time'])\n    \n    return data\n\n# 데이터 전처리 함수\ndef preprocess_data(data, point):\n    # 목표 변수 생성\n    data['target'] = (data['max_return_60min'] >= point).astype(int)\n\n    # 특성과 목표 변수 분리\n    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n    y = data['target']\n\n    # 무한대 값을 NaN으로 대체\n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # NaN 값을 평균으로 대체\n    imputer = SimpleImputer(strategy='mean')\n    X_imputed = imputer.fit_transform(X)\n\n    # 데이터 정규화\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X_imputed)\n\n    return X_scaled, y\n\n# 시계열 데이터 형태로 변환 함수\ndef create_sequences(data, target, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        label = target[i + sequence_length - 1]\n        sequences.append(seq)\n        targets.append(label)\n    return np.array(sequences), np.array(targets)\n\n# 예측 데이터를 시퀀스 형태로 변환\ndef create_sequences_for_prediction(data, sequence_length):\n    sequences = []\n    for i in range(len(data) - sequence_length + 1):\n        seq = data[i:i + sequence_length]\n        sequences.append(seq)\n    return np.array(sequences)\n\n# TCN 모델 정의\nclass TCNModel(nn.Module):\n    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n        super(TCNModel, self).__init__()\n        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_channels, 1)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)  # (batch_size, seq_len, input_size) -> (batch_size, input_size, seq_len)\n        y1 = self.tcn(x)\n        y1 = self.relu(y1)\n        y1 = self.dropout(y1)\n        y1 = y1[:, :, -1]\n        o = self.fc(y1)\n        return o\n    \ndef calculate_max_min_returns(df):\n    window_size = 120\n\n    # 'open_time' 열이 데이터프레임에 있는지 확인\n    if 'open_time' not in df.columns:\n        raise KeyError(\"'open_time' 열이 데이터프레임에 포함되어 있어야 합니다.\")\n\n    # 'open_time' 열을 datetime으로 변환\n    df['open_time'] = pd.to_datetime(df['open_time'])\n\n    # 인덱스 중복 확인 및 제거\n    df = df.drop_duplicates(subset='open_time', keep='first').copy()\n\n    # 인덱스를 'open_time'으로 설정\n    df.set_index('open_time', inplace=True)\n\n    # 현재 가격\n    current_price = df['close']\n\n    # 60분 윈도우를 적용하여 최대 및 최소 가격 계산\n    rolling_max = df['high'].rolling(window=window_size, min_periods=1).max().shift(-window_size)\n    rolling_min = df['low'].rolling(window=window_size, min_periods=1).min().shift(-window_size)\n\n    # 현재 행의 close와 60분 윈도우 내의 최대 high와 최소 low를 비교하여 수익률 계산\n    df['max_return_60min'] = ((rolling_max - current_price) / current_price) * 100\n    df['min_return_60min'] = ((rolling_min - current_price) / current_price) * 100\n\n    # 결측값을 적절히 처리 (예: 마지막 몇 행)\n    df['max_return_60min'].fillna(0, inplace=True)\n    df['min_return_60min'].fillna(0, inplace=True)\n\n    # 인덱스 리셋\n    df.reset_index(inplace=True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:38:46.870073Z","iopub.execute_input":"2024-08-10T12:38:46.870308Z","iopub.status.idle":"2024-08-10T12:38:46.886398Z","shell.execute_reply.started":"2024-08-10T12:38:46.870277Z","shell.execute_reply":"2024-08-10T12:38:46.885722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"for idx in range(5) :\n\n  print(f\"---------START({mention_List[idx]})------------\")\n\n  # 테스트 데이터 호출\n  data_test = pd.read_csv(Data_Test_list[idx])\n\n  # 최대 상승률과 최대 하락률 계산\n  data_test = calculate_max_min_returns(data_test)\n\n  #Unnamed: 0 열 제거\n  data_test = drop_unnamed_column(data_test)\n\n  # 테스트 데이터 시간 관련 열 변환\n  data_test = convert_time_features(data_test)\n\n  # 테스트 데이터 사용하지 않을 열 제외\n  data_test_predict = data_test.drop(columns=['max_return_60min', 'min_return_60min'])\n\n  # 테스트 데이터 무한대 값을 NaN으로 대체\n  data_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n  # 테스트 데이터 NaN 값을 평균으로 대체\n  imputer = SimpleImputer(strategy='mean')\n  data_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n\n  # 테스트 데이터 데이터 정규화\n  scaler = MinMaxScaler()\n  data_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n\n  # 데이터 호출\n  data = pd.read_csv(Data_list[idx])\n\n  # 최대 상승률과 최대 하락률 계산\n  data = calculate_max_min_returns(data)\n\n  # 시간 관련 열 변환\n  data = convert_time_features(data)\n\n  # 시퀀스 길이 설정\n  sequence_length = 60\n\n  # 예측용 시퀀스 데이터 생성\n  X_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n\n  #목표 수익\n  percent_point = Target_List[idx]\n\n  for idx2 in range(8) :\n    #목표 수익\n    percent_point = Target_List[idx2]\n\n    # 데이터 전처리\n    X_scaled, y = preprocess_data(data, percent_point)\n\n    # 데이터 길이 체크\n    if len(X_scaled) < sequence_length:\n        raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n\n    # 시퀀스 데이터 생성\n    y_array = y.values  # pandas Series를 numpy array로 변환\n    X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n\n    # 학습 데이터와 검증 데이터 분리\n    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n\n    # 데이터를 텐서로 변환\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n    # 데이터 로더 생성\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n    # 모델 설정\n    input_size = X_train.shape[2]\n    num_channels = 64\n    model = TCNModel(input_size, num_channels)\n\n    # 손실 함수 및 옵티마이저 설정\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # 조기 종료 설정\n    patience = 5\n    best_loss = float('inf')\n    patience_counter = 0\n\n    # 학습 및 검증 손실을 저장할 리스트\n    train_losses = []\n    val_losses = []\n\n    # 학습\n    num_epochs = 5  # 최대 에포크 수\n    for epoch in range(num_epochs):\n        # 학습 단계\n        model.train()\n        running_loss = 0.0\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        avg_train_loss = running_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # 검증 단계\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X_batch, y_batch in test_loader:\n                output = model(X_batch)\n                loss = criterion(output, y_batch)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(test_loader)\n        val_losses.append(avg_val_loss)\n\n        # 조기 종료 조건 체크\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n            break\n\n    # 모델 평가\n    model.eval()\n    with torch.no_grad():\n      y_true = []\n      y_pred = []\n      for X_batch, y_batch in test_loader:\n        output = model(X_batch)\n        y_true.extend(y_batch.tolist())\n        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n\n    # 이진 분류 결과를 위한 평가 지표 계산\n    y_pred = np.array(y_pred) > 0.5\n    y_true = y_test_tensor.numpy()\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n    print(f'F1 Score: {f1:.4f}')\n\n    # 모델 상태 저장\n    torch.save(model.state_dict(), Data_save[idx])  \n    \n    # 슬라이딩 윈도우로 데이터 범위 추출\n    num_rows = data_test_predict_scaled.shape[0]\n\n    # 시퀀스 데이터를 텐서로 변환\n    X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n\n    # 모델을 GPU로 이동 (가능한 경우)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    X_test_tensor = X_test_tensor.to(device)\n\n    #window_size = [43175, 86350, 129600, 172700, 216000, 259200, 302400] #3,4,5,6,7\n    window_size = [43175, 86350, 129600, 172700, 216000, 259200, 302400]\n    \n    #prediction 결과 저장\n    results = []\n\n    # 배치 크기 설정\n    batch_size = 63\n    \n    #\n    data_test_tmp = data_test\n    \n    for idx_window in range(4) :\n\n      #prediction 결과 저장\n      results = []\n\n      for end in tqdm(range(num_rows, window_size[idx_window] - 1, -batch_size)):\n          start = max(end - batch_size + 1, 0)\n          \n          # 해당 범위에 대한 시퀀스 텐서 추출\n          X_test_tensor_tmp = X_test_tensor[start:end]\n          \n          # 예측 수행\n          model.eval()\n          with torch.no_grad():\n              predictions = torch.sigmoid(model(X_test_tensor_tmp)).squeeze().cpu().numpy()\n\n          # 예측 결과를 이진 분류로 변환 (0 또는 1)\n          predictions = (predictions > 0.5).astype(int)\n          \n          # 예측 결과의 마지막 값을 추가\n          if len(predictions.shape) > 0:\n              results.append(predictions[-1])\n          else:\n              results.append(predictions)\n      \n      # 원래 순서대로 변경\n      results = results[::-1]  # 원래 순서대로 변경\n\n      data_test_tmp['Predictions'] = np.nan\n      data_test_tmp.loc[data_test_tmp.index[-len(results):], 'Predictions'] = results\n      data_test_tmp = data_test_tmp.dropna(subset=['Predictions'])\n\n      # 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\n      count_max_return_ge_1_prediction_0 = len(data_test_tmp[(data_test_tmp['max_return_60min'] >= 1.1) & (data_test_tmp['Predictions'] == 1)])\n\n      # 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\n      count_max_return_lt_1_prediction_1 = len(data_test_tmp[(data_test_tmp['max_return_60min'] < 1.1) & (data_test_tmp['Predictions'] == 0)])\n      \n      #\n      tmp1 = count_max_return_ge_1_prediction_0/len(data_test_tmp)*100\n      tmp2 = count_max_return_lt_1_prediction_1/len(data_test_tmp)*100\n        \n      #\n      print(f\"[max_return_60min/{mention_List[idx]}/{percent_point}이상/1/{window_size[idx_window]}] : {tmp1}\")\n      print(f\"[max_return_60min/{mention_List[idx]}/{percent_point}미만/0/{window_size[idx_window]}] : {tmp2}\")\n      print(f\"[확률/{mention_List[idx]}/{percent_point}/{window_size[idx_window]}] : {tmp1 + tmp2}\")\n\n\n  print(f\"---------END({mention_List[idx]})------------\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:38:46.888069Z","iopub.execute_input":"2024-08-10T12:38:46.888346Z","iopub.status.idle":"2024-08-10T13:17:54.194901Z","shell.execute_reply.started":"2024-08-10T12:38:46.888320Z","shell.execute_reply":"2024-08-10T13:17:54.193884Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"---------START(3m)------------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_13/4033693619.py:111: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['max_return_60min'].fillna(0, inplace=True)\n/tmp/ipykernel_13/4033693619.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['min_return_60min'].fillna(0, inplace=True)\n/tmp/ipykernel_13/4033693619.py:111: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['max_return_60min'].fillna(0, inplace=True)\n/tmp/ipykernel_13/4033693619.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['min_return_60min'].fillna(0, inplace=True)\n/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.7426\nPrecision: 0.7426\nRecall: 1.0000\nF1 Score: 0.8523\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 839.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.7이상/1/43175] : 60.69391634980988\n[max_return_60min/3m/0.7미만/0/43175] : 0.0\n[확률/3m/0.7/43175] : 60.69391634980988\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 843.93it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.7이상/1/86350] : 62.55289139633287\n[max_return_60min/3m/0.7미만/0/86350] : 0.0\n[확률/3m/0.7/86350] : 62.55289139633287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 872.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.7이상/1/129600] : 65.30054644808743\n[max_return_60min/3m/0.7미만/0/129600] : 0.0\n[확률/3m/0.7/129600] : 65.30054644808743\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 911.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.7이상/1/172700] : 0.0\n[max_return_60min/3m/0.7미만/0/172700] : 0.0\n[확률/3m/0.7/172700] : 0.0\nAccuracy: 0.6845\nPrecision: 0.6884\nRecall: 0.9777\nF1 Score: 0.8080\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 716.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.9이상/1/43175] : 60.45627376425855\n[max_return_60min/3m/0.9미만/0/43175] : 0.0\n[확률/3m/0.9/43175] : 60.45627376425855\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 886.72it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.9이상/1/86350] : 62.200282087447114\n[max_return_60min/3m/0.9미만/0/86350] : 0.0\n[확률/3m/0.9/86350] : 62.200282087447114\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 871.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.9이상/1/129600] : 64.61748633879782\n[max_return_60min/3m/0.9미만/0/129600] : 0.0\n[확률/3m/0.9/129600] : 64.61748633879782\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 863.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/0.9이상/1/172700] : 0.0\n[max_return_60min/3m/0.9미만/0/172700] : 0.0\n[확률/3m/0.9/172700] : 0.0\nAccuracy: 0.6610\nPrecision: 0.6818\nRecall: 0.8520\nF1 Score: 0.7575\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 703.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.1이상/1/43175] : 55.418250950570346\n[max_return_60min/3m/1.1미만/0/43175] : 1.85361216730038\n[확률/3m/1.1/43175] : 57.27186311787073\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 909.98it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.1이상/1/86350] : 55.00705218617772\n[max_return_60min/3m/1.1미만/0/86350] : 2.2566995768688294\n[확률/3m/1.1/86350] : 57.26375176304654\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 889.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.1이상/1/129600] : 51.63934426229508\n[max_return_60min/3m/1.1미만/0/129600] : 3.278688524590164\n[확률/3m/1.1/129600] : 54.91803278688525\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 892.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.1이상/1/172700] : 0.0\n[max_return_60min/3m/1.1미만/0/172700] : 10.416666666666668\n[확률/3m/1.1/172700] : 10.416666666666668\nAccuracy: 0.6471\nPrecision: 0.6561\nRecall: 0.7973\nF1 Score: 0.7198\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 701.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.3이상/1/43175] : 53.564638783269956\n[max_return_60min/3m/1.3미만/0/43175] : 3.2319391634980987\n[확률/3m/1.3/43175] : 56.79657794676805\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 902.41it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.3이상/1/86350] : 52.67983074753173\n[max_return_60min/3m/1.3미만/0/86350] : 4.231311706629055\n[확률/3m/1.3/86350] : 56.91114245416079\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 904.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.3이상/1/129600] : 48.224043715846996\n[max_return_60min/3m/1.3미만/0/129600] : 4.781420765027322\n[확률/3m/1.3/129600] : 53.005464480874316\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 917.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.3이상/1/172700] : 0.0\n[max_return_60min/3m/1.3미만/0/172700] : 14.583333333333334\n[확률/3m/1.3/172700] : 14.583333333333334\nAccuracy: 0.6411\nPrecision: 0.6333\nRecall: 0.7407\nF1 Score: 0.6828\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 710.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.5이상/1/43175] : 48.336501901140686\n[max_return_60min/3m/1.5미만/0/43175] : 6.701520912547529\n[확률/3m/1.5/43175] : 55.038022813688215\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 865.50it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.5이상/1/86350] : 45.41607898448519\n[max_return_60min/3m/1.5미만/0/86350] : 8.603667136812412\n[확률/3m/1.5/86350] : 54.0197461212976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 815.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.5이상/1/129600] : 39.61748633879781\n[max_return_60min/3m/1.5미만/0/129600] : 8.60655737704918\n[확률/3m/1.5/129600] : 48.22404371584699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 774.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.5이상/1/172700] : 0.0\n[max_return_60min/3m/1.5미만/0/172700] : 22.916666666666664\n[확률/3m/1.5/172700] : 22.916666666666664\nAccuracy: 0.6427\nPrecision: 0.6226\nRecall: 0.6379\nF1 Score: 0.6301\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:03<00:00, 686.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.7이상/1/43175] : 42.30038022813688\n[max_return_60min/3m/1.7미만/0/43175] : 12.357414448669202\n[확률/3m/1.7/43175] : 54.657794676806084\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 848.01it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.7이상/1/86350] : 37.44710860366713\n[max_return_60min/3m/1.7미만/0/86350] : 15.937940761636108\n[확률/3m/1.7/86350] : 53.38504936530324\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 835.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.7이상/1/129600] : 30.05464480874317\n[max_return_60min/3m/1.7미만/0/129600] : 16.666666666666664\n[확률/3m/1.7/129600] : 46.721311475409834\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 849.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.7이상/1/172700] : 0.0\n[max_return_60min/3m/1.7미만/0/172700] : 50.0\n[확률/3m/1.7/172700] : 50.0\nAccuracy: 0.6529\nPrecision: 0.6358\nRecall: 0.4829\nF1 Score: 0.5489\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:03<00:00, 701.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.9이상/1/43175] : 31.1787072243346\n[max_return_60min/3m/1.9미만/0/43175] : 20.389733840304185\n[확률/3m/1.9/43175] : 51.56844106463879\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 840.50it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.9이상/1/86350] : 25.035260930888576\n[max_return_60min/3m/1.9미만/0/86350] : 24.61212976022567\n[확률/3m/1.9/86350] : 49.647390691114246\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 860.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.9이상/1/129600] : 15.573770491803279\n[max_return_60min/3m/1.9미만/0/129600] : 25.136612021857925\n[확률/3m/1.9/129600] : 40.7103825136612\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 920.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/1.9이상/1/172700] : 0.0\n[max_return_60min/3m/1.9미만/0/172700] : 66.66666666666666\n[확률/3m/1.9/172700] : 66.66666666666666\nAccuracy: 0.6700\nPrecision: 0.6415\nRecall: 0.4019\nF1 Score: 0.4942\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2104/2104 [00:02<00:00, 708.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/2.1이상/1/43175] : 22.24334600760456\n[max_return_60min/3m/2.1미만/0/43175] : 27.138783269961976\n[확률/3m/2.1/43175] : 49.382129277566534\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1418/1418 [00:01<00:00, 905.39it/s]\n/tmp/ipykernel_13/1453210920.py:215: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_test_tmp['Predictions'] = np.nan\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/2.1이상/1/86350] : 16.502115655853313\n[max_return_60min/3m/2.1미만/0/86350] : 29.26657263751763\n[확률/3m/2.1/86350] : 45.76868829337094\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 732/732 [00:00<00:00, 919.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/2.1이상/1/129600] : 7.5136612021857925\n[max_return_60min/3m/2.1미만/0/129600] : 29.78142076502732\n[확률/3m/2.1/129600] : 37.295081967213115\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:00<00:00, 933.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"[max_return_60min/3m/2.1이상/1/172700] : 0.0\n[max_return_60min/3m/2.1미만/0/172700] : 79.16666666666666\n[확률/3m/2.1/172700] : 79.16666666666666\n---------END(3m)------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m) :\n\u001b[0;32m----> 3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------START(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmention_List\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;66;03m# 테스트 데이터 호출\u001b[39;00m\n\u001b[1;32m      6\u001b[0m   data_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Data_Test_list[idx])\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]}]}