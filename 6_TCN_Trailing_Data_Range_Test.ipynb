{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7BbevEP4ycxxOOFDIMICq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV6_k62oAkHN"
      },
      "outputs": [],
      "source": [
        "pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tdqm"
      ],
      "metadata": {
        "id": "RoGW75qSA9TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TensorFlow 및 TPU 설정\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "7ytyAJBuA_gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 호출 시리즈\n",
        "Data_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_1m_Micro_Indicator3.csv',\n",
        "             '/kaggle/input/data-set-24-08-09/SOL_Data_3m_Indicator3.csv',\n",
        "             '/kaggle/input/data-set-24-08-09/SOL_Data_5m_Indicator3.csv',\n",
        "             '/kaggle/input/data-set-24-08-09/SOL_Data_15m_Indicator3.csv',\n",
        "             '/kaggle/input/data-set-24-08-09/SOL_Data_30m_Indicator3.csv']\n",
        "\n",
        "Data_save = ['/kaggle/working/SOL_Data_1m_Micro_INDICATOR3_TCN_v4.pth',\n",
        "             '/kaggle/working/SOL_Data_3m_Micro_INDICATOR3_TCN_v4.pth',\n",
        "             '/kaggle/working/SOL_Data_5m_Micro_INDICATOR3_TCN_v4.pth',\n",
        "             '/kaggle/working/SOL_Data_15m_Micro_INDICATOR3_TCN_v4.pth',\n",
        "             '/kaggle/working/SOL_Data_30m_Micro_INDICATOR3_TCN_v4.pth']\n",
        "\n",
        "Data_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_1m_Indicator3.csv',\n",
        "                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_3m_Indicator3.csv',\n",
        "                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_5m_Indicator3.csv',\n",
        "                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_15m_Indicator3.csv',\n",
        "                  '/kaggle/input/data-set-24-08-09/SOL_Data_Test_30m_Indicator3.csv']\n",
        "\n",
        "Target_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n",
        "\n",
        "mention_List = [\"1m\", \"3m\", \"5m\", \"15m\", \"30m\"]"
      ],
      "metadata": {
        "id": "vfNbIayRBAKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 호출 시리즈\n",
        "Data_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_1m_Micro_Indicator3.csv']\n",
        "\n",
        "Data_save = ['/kaggle/working/SOL_Data_1m_INDICATOR3_TCN_v4.pth']\n",
        "\n",
        "Data_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_1m_Indicator3.csv']\n",
        "\n",
        "Target_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n",
        "\n",
        "mention_List = [\"1m\"]"
      ],
      "metadata": {
        "id": "QpxIbLfyBCOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 호출 시리즈\n",
        "Data_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_3m_Indicator3.csv']\n",
        "\n",
        "Data_save = ['/kaggle/working/SOL_Data_3m_INDICATOR3_TCN_v4.pth']\n",
        "\n",
        "Data_Test_list = ['/kaggle/input/data-set-24-08-09/SOL_Data_Test_3m_Indicator3.csv']\n",
        "\n",
        "Target_List = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1]\n",
        "\n",
        "mention_List = [\"3m\"]"
      ],
      "metadata": {
        "id": "lNsKEn1wBD0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#함수 시리즈\n",
        "def drop_unnamed_column(df):\n",
        "    # 'Unnamed: 0' 열이 존재하는지 확인\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        # 존재한다면 해당 열 삭제\n",
        "        df = df.drop(columns=['Unnamed: 0'])\n",
        "    return df\n",
        "\n",
        "# 시간 관련 열 변환 함수\n",
        "def convert_time_features(data):\n",
        "    # open_time 열이 datetime 형식이 아닌 경우 변환\n",
        "    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n",
        "        data['open_time'] = pd.to_datetime(data['open_time'])\n",
        "\n",
        "    # time 열을 분 단위로 변환\n",
        "    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n",
        "\n",
        "    # 사용하지 않을 열 제외\n",
        "    data = data.drop(columns=['open_time'])\n",
        "\n",
        "    return data\n",
        "\n",
        "# 데이터 전처리 함수\n",
        "def preprocess_data(data, point):\n",
        "    # 목표 변수 생성\n",
        "    data['target'] = (data['max_return_60min'] >= point).astype(int)\n",
        "\n",
        "    # 특성과 목표 변수 분리\n",
        "    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n",
        "    y = data['target']\n",
        "\n",
        "    # 무한대 값을 NaN으로 대체\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # NaN 값을 평균으로 대체\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# 시계열 데이터 형태로 변환 함수\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        label = target[i + sequence_length - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 예측 데이터를 시퀀스 형태로 변환\n",
        "def create_sequences_for_prediction(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "# TCN 모델 정의\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TCNModel, self).__init__()\n",
        "        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # (batch_size, seq_len, input_size) -> (batch_size, input_size, seq_len)\n",
        "        y1 = self.tcn(x)\n",
        "        y1 = self.relu(y1)\n",
        "        y1 = self.dropout(y1)\n",
        "        y1 = y1[:, :, -1]\n",
        "        o = self.fc(y1)\n",
        "        return o\n",
        "\n",
        "def calculate_max_min_returns(df):\n",
        "    window_size = 120\n",
        "\n",
        "    # 'open_time' 열이 데이터프레임에 있는지 확인\n",
        "    if 'open_time' not in df.columns:\n",
        "        raise KeyError(\"'open_time' 열이 데이터프레임에 포함되어 있어야 합니다.\")\n",
        "\n",
        "    # 'open_time' 열을 datetime으로 변환\n",
        "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
        "\n",
        "    # 인덱스 중복 확인 및 제거\n",
        "    df = df.drop_duplicates(subset='open_time', keep='first').copy()\n",
        "\n",
        "    # 인덱스를 'open_time'으로 설정\n",
        "    df.set_index('open_time', inplace=True)\n",
        "\n",
        "    # 현재 가격\n",
        "    current_price = df['close']\n",
        "\n",
        "    # 60분 윈도우를 적용하여 최대 및 최소 가격 계산\n",
        "    rolling_max = df['high'].rolling(window=window_size, min_periods=1).max().shift(-window_size)\n",
        "    rolling_min = df['low'].rolling(window=window_size, min_periods=1).min().shift(-window_size)\n",
        "\n",
        "    # 현재 행의 close와 60분 윈도우 내의 최대 high와 최소 low를 비교하여 수익률 계산\n",
        "    df['max_return_60min'] = ((rolling_max - current_price) / current_price) * 100\n",
        "    df['min_return_60min'] = ((rolling_min - current_price) / current_price) * 100\n",
        "\n",
        "    # 결측값을 적절히 처리 (예: 마지막 몇 행)\n",
        "    df['max_return_60min'].fillna(0, inplace=True)\n",
        "    df['min_return_60min'].fillna(0, inplace=True)\n",
        "\n",
        "    # 인덱스 리셋\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Fobr3JoDBG7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(5) :\n",
        "\n",
        "  print(f\"---------START({mention_List[idx]})------------\")\n",
        "\n",
        "  # 테스트 데이터 호출\n",
        "  data_test = pd.read_csv(Data_Test_list[idx])\n",
        "\n",
        "  # 최대 상승률과 최대 하락률 계산\n",
        "  data_test = calculate_max_min_returns(data_test)\n",
        "\n",
        "  #Unnamed: 0 열 제거\n",
        "  data_test = drop_unnamed_column(data_test)\n",
        "\n",
        "  # 테스트 데이터 시간 관련 열 변환\n",
        "  data_test = convert_time_features(data_test)\n",
        "\n",
        "  # 테스트 데이터 사용하지 않을 열 제외\n",
        "  data_test_predict = data_test.drop(columns=['max_return_60min', 'min_return_60min'])\n",
        "\n",
        "  # 테스트 데이터 무한대 값을 NaN으로 대체\n",
        "  data_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "  # 테스트 데이터 NaN 값을 평균으로 대체\n",
        "  imputer = SimpleImputer(strategy='mean')\n",
        "  data_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n",
        "\n",
        "  # 테스트 데이터 데이터 정규화\n",
        "  scaler = MinMaxScaler()\n",
        "  data_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n",
        "\n",
        "  # 데이터 호출\n",
        "  data = pd.read_csv(Data_list[idx])\n",
        "\n",
        "  # 최대 상승률과 최대 하락률 계산\n",
        "  data = calculate_max_min_returns(data)\n",
        "\n",
        "  # 시간 관련 열 변환\n",
        "  data = convert_time_features(data)\n",
        "\n",
        "  # 시퀀스 길이 설정\n",
        "  sequence_length = 60\n",
        "\n",
        "  # 예측용 시퀀스 데이터 생성\n",
        "  X_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n",
        "\n",
        "  #목표 수익\n",
        "  percent_point = Target_List[idx]\n",
        "\n",
        "  for idx2 in range(8) :\n",
        "    #목표 수익\n",
        "    percent_point = Target_List[idx2]\n",
        "\n",
        "    # 데이터 전처리\n",
        "    X_scaled, y = preprocess_data(data, percent_point)\n",
        "\n",
        "    # 데이터 길이 체크\n",
        "    if len(X_scaled) < sequence_length:\n",
        "        raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n",
        "\n",
        "    # 시퀀스 데이터 생성\n",
        "    y_array = y.values  # pandas Series를 numpy array로 변환\n",
        "    X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "    # 학습 데이터와 검증 데이터 분리\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 데이터를 텐서로 변환\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # 모델 설정\n",
        "    input_size = X_train.shape[2]\n",
        "    num_channels = 64\n",
        "    model = TCNModel(input_size, num_channels)\n",
        "\n",
        "    # 손실 함수 및 옵티마이저 설정\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 조기 종료 설정\n",
        "    patience = 5\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # 학습 및 검증 손실을 저장할 리스트\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # 학습\n",
        "    num_epochs = 5  # 최대 에포크 수\n",
        "    for epoch in range(num_epochs):\n",
        "        # 학습 단계\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # 검증 단계\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                output = model(X_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # 조기 종료 조건 체크\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n",
        "            break\n",
        "\n",
        "    # 모델 평가\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "      for X_batch, y_batch in test_loader:\n",
        "        output = model(X_batch)\n",
        "        y_true.extend(y_batch.tolist())\n",
        "        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n",
        "\n",
        "    # 이진 분류 결과를 위한 평가 지표 계산\n",
        "    y_pred = np.array(y_pred) > 0.5\n",
        "    y_true = y_test_tensor.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "    # 모델 상태 저장\n",
        "    torch.save(model.state_dict(), Data_save[idx])\n",
        "\n",
        "    # 슬라이딩 윈도우로 데이터 범위 추출\n",
        "    num_rows = data_test_predict_scaled.shape[0]\n",
        "\n",
        "    # 시퀀스 데이터를 텐서로 변환\n",
        "    X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
        "\n",
        "    # 모델을 GPU로 이동 (가능한 경우)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    X_test_tensor = X_test_tensor.to(device)\n",
        "\n",
        "    #window_size = [43175, 86350, 129600, 172700, 216000, 259200, 302400] #3,4,5,6,7\n",
        "    window_size = [43175, 86350, 129600, 172700, 216000, 259200, 302400]\n",
        "\n",
        "    #prediction 결과 저장\n",
        "    results = []\n",
        "\n",
        "    # 배치 크기 설정\n",
        "    batch_size = 63\n",
        "\n",
        "    #\n",
        "    data_test_tmp = data_test\n",
        "\n",
        "    for idx_window in range(4) :\n",
        "\n",
        "      #prediction 결과 저장\n",
        "      results = []\n",
        "\n",
        "      for end in tqdm(range(num_rows, window_size[idx_window] - 1, -batch_size)):\n",
        "          start = max(end - batch_size + 1, 0)\n",
        "\n",
        "          # 해당 범위에 대한 시퀀스 텐서 추출\n",
        "          X_test_tensor_tmp = X_test_tensor[start:end]\n",
        "\n",
        "          # 예측 수행\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              predictions = torch.sigmoid(model(X_test_tensor_tmp)).squeeze().cpu().numpy()\n",
        "\n",
        "          # 예측 결과를 이진 분류로 변환 (0 또는 1)\n",
        "          predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "          # 예측 결과의 마지막 값을 추가\n",
        "          if len(predictions.shape) > 0:\n",
        "              results.append(predictions[-1])\n",
        "          else:\n",
        "              results.append(predictions)\n",
        "\n",
        "      # 원래 순서대로 변경\n",
        "      results = results[::-1]  # 원래 순서대로 변경\n",
        "\n",
        "      data_test_tmp['Predictions'] = np.nan\n",
        "      data_test_tmp.loc[data_test_tmp.index[-len(results):], 'Predictions'] = results\n",
        "      data_test_tmp = data_test_tmp.dropna(subset=['Predictions'])\n",
        "\n",
        "      # 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\n",
        "      count_max_return_ge_1_prediction_0 = len(data_test_tmp[(data_test_tmp['max_return_60min'] >= 1.1) & (data_test_tmp['Predictions'] == 1)])\n",
        "\n",
        "      # 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\n",
        "      count_max_return_lt_1_prediction_1 = len(data_test_tmp[(data_test_tmp['max_return_60min'] < 1.1) & (data_test_tmp['Predictions'] == 0)])\n",
        "\n",
        "      #\n",
        "      tmp1 = count_max_return_ge_1_prediction_0/len(data_test_tmp)*100\n",
        "      tmp2 = count_max_return_lt_1_prediction_1/len(data_test_tmp)*100\n",
        "\n",
        "      #\n",
        "      print(f\"[max_return_60min/{mention_List[idx]}/{percent_point}이상/1/{window_size[idx_window]}] : {tmp1}\")\n",
        "      print(f\"[max_return_60min/{mention_List[idx]}/{percent_point}미만/0/{window_size[idx_window]}] : {tmp2}\")\n",
        "      print(f\"[확률/{mention_List[idx]}/{percent_point}/{window_size[idx_window]}] : {tmp1 + tmp2}\")\n",
        "\n",
        "\n",
        "  print(f\"---------END({mention_List[idx]})------------\")"
      ],
      "metadata": {
        "id": "bj6eKWvbBKOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}