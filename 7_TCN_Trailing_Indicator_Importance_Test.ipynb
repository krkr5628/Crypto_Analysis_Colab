{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4jWjUE1gcZFvZr8JaKL2y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "id": "WvQ75ycqfGJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2eadb2-b51e-4bde-a249-b18595f19cbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bJpt_Yo-cvnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06c802d-9067-4899-c315-168c66dd8ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tdqm\n",
            "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tdqm) (4.66.5)\n",
            "Building wheels for collected packages: tdqm\n",
            "  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1321 sha256=e59d3d61a9368d87b191978eceb840e1b1387aab5b60fd161ca21b64a59f5f41\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/31/b8/7b711038035720ba0df14376af06e5e76b9bd61759c861ad92\n",
            "Successfully built tdqm\n",
            "Installing collected packages: tdqm\n",
            "Successfully installed tdqm-0.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install tdqm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TensorFlow 및 TPU 설정\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "lwRdx7OpfHYN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wy00fePumIYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4590a519-cd06-480a-d668-a2114cabd95e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TCN 모델 정의\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TCNModel, self).__init__()\n",
        "        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # (batch_size, seq_len, input_size) -> (batch_size, input_size, seq_len)\n",
        "        y1 = self.tcn(x)\n",
        "        y1 = self.relu(y1)\n",
        "        y1 = self.dropout(y1)\n",
        "        y1 = y1[:, :, -1]\n",
        "        o = self.fc(y1)\n",
        "        return o"
      ],
      "metadata": {
        "id": "sDKK_avr0cR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1 1분 6개\n",
        "#Main\n",
        "file_path1 = '/content/drive/MyDrive/Data/SOL_Data_1m_Micro_Indicator3.csv'\n",
        "data = pd.read_csv(file_path1)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qw845_D2l_-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시간 관련 열 변환 함수\n",
        "def convert_time_features(data):\n",
        "    # open_time 열이 datetime 형식이 아닌 경우 변환\n",
        "    if not np.issubdtype(data['open_time'].dtype, np.datetime64):\n",
        "        data['open_time'] = pd.to_datetime(data['open_time'])\n",
        "\n",
        "    # time 열을 분 단위로 변환\n",
        "    data['time'] = data['open_time'].dt.hour * 60 + data['open_time'].dt.minute\n",
        "\n",
        "    # 사용하지 않을 열 제외\n",
        "    #data = data.drop(columns=['open_time', 'Unnamed: 0'])\n",
        "    data = data.drop(columns=['open_time'])\n",
        "\n",
        "    return data\n",
        "\n",
        "# 시간 관련 열 변환\n",
        "data = convert_time_features(data)\n",
        "\n",
        "#Target 생\n",
        "data['target'] = (data['max_return_60min'] >= 1.1).astype(int)"
      ],
      "metadata": {
        "id": "SRjX3L07rKXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1 1분 6개\n",
        "#Test\n",
        "file_path2 = '/content/drive/MyDrive/Data/SOL_Data_Test_1m_Indicator3.csv'\n",
        "data_test = pd.read_csv(file_path2)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "-YKodO_5qPhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open_time 열을 datetime 형식으로 변환\n",
        "if not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n",
        "    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n",
        "\n",
        "# time 열을 분 단위로 변환\n",
        "data_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute"
      ],
      "metadata": {
        "id": "ILMj7CdF0f9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indicators_list = [\n",
        "    \"volume_ma_100\", \"volume_ma_5\", \"volume\", \"volume_ma_50\", \"obv\", \"volume_ma_20\", \"volume_ma_200\", \"atr_14\", \"disparity_index_50\", \"disparity_index_5\",\n",
        "    \"atr_10\", \"bollinger_hband_100\", \"disparity_index_10\", \"supertrend_lower_14_2_10\", \"bollinger_lband_10\", \"bollinger_hband_200\", \"atr_5\", \"bollinger_lband_100\", \"bollinger_lband_200\", \"price_ma_100\",\n",
        "    \"ichimoku_conversion_52\", \"disparity_index_20\", \"lowerband\", \"atr_50\", \"price_ma_200\", \"ichimoku_conversion_200\", \"atr_20\", \"ichimoku_conversion_100\", \"volume_ma_10\", \"supertrend_lower_20_4_50\",\n",
        "    \"ichimoku_base_9\", \"supertrend_upper_10_3_20\", \"Accumulation_Distribution_Line\", \"bollinger_lband_50\", \"bollinger_hband_50\", \"bollinger_lband_20\", \"bollinger_hband_20\", \"disparity_index_200\", \"upperband\", \"price_ma_50\",\n",
        "    \"low\", \"time\", \"disparity_index_100\", \"Parabolic_SAR_0.06\", \"supertrend_upper_20_4_50\", \"supertrend_upper_7_3_14\", \"supertrend_lower_10_3_20\", \"ichimoku_conversion_9\", \"price_ma_10\", \"supertrend_upper_14_2_10\",\n",
        "    \"vwap\", \"Parabolic_SAR_0.1\", \"ROC_50\", \"supertrend_in_uptrend_10_3_20\", \"MFI_50\", \"supertrend_lower_7_3_14\", \"supertrend_in_uptrend_20_4_50\", \"close\", \"bollinger_hband_10\", \"Momentum_50\",\n",
        "    \"VR_50\", \"Elder_Force_Index_25\", \"price_ma_20\", \"in_uptrend\", \"price_ma_5\", \"open\", \"supertrend_in_uptrend_7_3_14\", \"VR_40\", \"Momentum_30\", \"MFI_40\",\n",
        "    \"stoch_%k_20_7\", \"high\", \"stoch_%k_9_3\", \"CMO_40\", \"ROC_30\", \"CMO_50\", \"Parabolic_SAR_0.02\", \"stoch_%d_21_5\", \"Williams_%R_40\", \"Parabolic_SAR_0.04\",\n",
        "    \"CMO_20\", \"Williams_%R_50\", \"CMO_10\", \"ROC_20\", \"ROC_40\", \"MFI_30\", \"VR_30\", \"CCI_50\", \"Momentum_40\", \"Williams_%R_30\",\n",
        "    \"MFI_20\", \"Momentum_10\", \"ROC_10\", \"MFI_10\", \"stoch_%d_20_7\", \"Elder_Force_Index_10\", \"Elder_Force_Index_13\", \"CCI_30\", \"VR_20\", \"CMO_30\",\n",
        "    \"VR_10\", \"CCI_40\", \"CCI_10\", \"supertrend_in_uptrend_14_2_10\", \"Momentum_20\", \"stoch_%k_21_5\", \"stoch_%d_9_3\", \"stoch_%d_14_3\", \"CCI_20\", \"stoch_%k_14_3\",\n",
        "    \"Elder_Force_Index_2\", \"Elder_Force_Index_5\", \"stoch_%d_5_2\", \"stoch_%k_5_2\", \"Parabolic_SAR_0.08\", \"Williams_%R_20\", \"supertrend_lower_50_5_5\", \"supertrend_in_uptrend_50_5_5\", \"Williams_%R_10\", \"supertrend_upper_50_5_5\",\n",
        "    \"RSI_40\", \"RSI_30\", \"RSI_20\", \"RSI_10\", \"atr\", \"RSI_50\", \"Relative_Vigor_Index_10\", \"ichimoku_base_200\", \"Relative_Vigor_Index_50\", \"Relative_Vigor_Index_20\",\n",
        "    \"Relative_Vigor_Index_30\", \"Relative_Vigor_Index_40\", \"ichimoku_base_52\", \"ichimoku_conversion_26\", \"ichimoku_base_26\", \"ichimoku_base_100\"\n",
        "]"
      ],
      "metadata": {
        "id": "25WuKIwohnEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "for idx in range(20, len(indicators_list) + 1) :\n",
        "\n",
        "  indicator_tmp = indicators_list[:idx]\n",
        "\n",
        "  print(f\"---------START({indicator_tmp[len(indicator_tmp) - 1]})------------\")\n",
        "\n",
        "  # 특성과 목표 변수 분리\n",
        "  X = data[indicator_tmp]\n",
        "  y = data['target']\n",
        "\n",
        "  # 무한대 값을 NaN으로 대체\n",
        "  X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "  # NaN 값을 평균으로 대체\n",
        "  imputer = SimpleImputer(strategy='mean')\n",
        "  X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "  # 데이터 정규화\n",
        "  scaler = MinMaxScaler()\n",
        "  X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "  # 예측 데이터를 시퀀스 형태로 변환\n",
        "  def create_sequences_for_prediction(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "  # 시퀀스 데이터 생성\n",
        "  y_array = y.values\n",
        "  X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "  # 피처들만 남기기\n",
        "  data_test_predict = data_test[indicator_tmp]\n",
        "\n",
        "  # 무한대 값을 NaN으로 대체\n",
        "  data_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "  # NaN 값을 평균으로 대체\n",
        "  imputer = SimpleImputer(strategy='mean')\n",
        "  data_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n",
        "\n",
        "  # 데이터 정규화\n",
        "  scaler = MinMaxScaler()\n",
        "  data_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n",
        "\n",
        "  # 예측 데이터를 시퀀스 형태로 변환 (LSTM용)\n",
        "  def create_sequences_for_prediction(data, sequence_length):\n",
        "      sequences = []\n",
        "      for i in range(len(data) - sequence_length + 1):\n",
        "          seq = data[i:i + sequence_length]\n",
        "          sequences.append(seq)\n",
        "      return np.array(sequences)\n",
        "\n",
        "  # 예측용 시퀀스 데이터 생성\n",
        "  X_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)\n",
        "\n",
        "  # 학습 데이터와 검증 데이터 분리\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "  # 데이터를 텐서로 변환\n",
        "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "  y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "  y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "  # 데이터 로더 생성\n",
        "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "  test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "  # 모델 설정\n",
        "  input_size = X_train.shape[2]\n",
        "  num_channels = 64\n",
        "  model = TCNModel(input_size, num_channels)\n",
        "\n",
        "  # 손실 함수 및 옵티마이저 설정\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # 조기 종료 설정\n",
        "  patience = 5\n",
        "  best_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "\n",
        "  # 학습 및 검증 손실을 저장할 리스트\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  # 학습\n",
        "  num_epochs = 5  # 최대 에포크 수\n",
        "  for epoch in range(num_epochs):\n",
        "      # 학습 단계\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for X_batch, y_batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          output = model(X_batch)\n",
        "          loss = criterion(output, y_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      avg_train_loss = running_loss / len(train_loader)\n",
        "      train_losses.append(avg_train_loss)\n",
        "\n",
        "      # 검증 단계\n",
        "      model.eval()\n",
        "      val_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "          for X_batch, y_batch in test_loader:\n",
        "              output = model(X_batch)\n",
        "              loss = criterion(output, y_batch)\n",
        "              val_loss += loss.item()\n",
        "\n",
        "      avg_val_loss = val_loss / len(test_loader)\n",
        "      val_losses.append(avg_val_loss)\n",
        "\n",
        "      # 조기 종료 조건 체크\n",
        "      if avg_val_loss < best_loss:\n",
        "          best_loss = avg_val_loss\n",
        "          patience_counter = 0\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "\n",
        "      if patience_counter >= patience:\n",
        "          print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n",
        "          break\n",
        "\n",
        "  # 모델 평가\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "      output = model(X_batch)\n",
        "      y_true.extend(y_batch.tolist())\n",
        "      y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n",
        "\n",
        "  # 이진 분류 결과를 위한 평가 지표 계산\n",
        "  y_pred = np.array(y_pred) > 0.5\n",
        "  y_true = y_test_tensor.numpy()\n",
        "\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "  print(f'Accuracy: {accuracy:.4f}')\n",
        "  print(f'Precision: {precision:.4f}')\n",
        "  print(f'Recall: {recall:.4f}')\n",
        "  print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "  # 슬라이딩 윈도우로 데이터 범위 추출\n",
        "  num_rows = data_test_predict_scaled.shape[0]\n",
        "\n",
        "  # 시퀀스 데이터를 텐서로 변환\n",
        "  X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
        "\n",
        "  # 모델을 GPU로 이동 (가능한 경우)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "  X_test_tensor = X_test_tensor.to(device)\n",
        "\n",
        "  #prediction 결과 저장\n",
        "  results = []\n",
        "\n",
        "  # 배치 크기 설정\n",
        "  batch_size = 63\n",
        "\n",
        "  #\n",
        "  data_test_tmp = data_test\n",
        "\n",
        "  #prediction 결과 저장\n",
        "  results = []\n",
        "\n",
        "  for end in tqdm(range(num_rows, 259200 - 1, -batch_size)):\n",
        "      start = max(end - batch_size + 1, 0)\n",
        "\n",
        "      # 해당 범위에 대한 시퀀스 텐서 추출\n",
        "      X_test_tensor_tmp = X_test_tensor[start:end]\n",
        "\n",
        "      # 예측 수행\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          predictions = torch.sigmoid(model(X_test_tensor_tmp)).squeeze().cpu().numpy()\n",
        "\n",
        "      # 예측 결과를 이진 분류로 변환 (0 또는 1)\n",
        "      predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "      # 예측 결과의 마지막 값을 추가\n",
        "      if len(predictions.shape) > 0:\n",
        "              results.append(predictions[-1])\n",
        "      else:\n",
        "          results.append(predictions)\n",
        "\n",
        "  # 원래 순서대로 변경\n",
        "  results = results[::-1]  # 원래 순서대로 변경\n",
        "\n",
        "  data_test_tmp['Predictions'] = np.nan\n",
        "  data_test_tmp.loc[data_test_tmp.index[-len(results):], 'Predictions'] = results\n",
        "  data_test_tmp = data_test_tmp.dropna(subset=['Predictions'])\n",
        "\n",
        "  # 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\n",
        "  count_max_return_ge_1_prediction_0 = len(data_test_tmp[(data_test_tmp['max_return_60min'] >= 1.1) & (data_test_tmp['Predictions'] == 1)])\n",
        "\n",
        "  # 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\n",
        "  count_max_return_lt_1_prediction_1 = len(data_test_tmp[(data_test_tmp['max_return_60min'] < 1.1) & (data_test_tmp['Predictions'] == 0)])\n",
        "\n",
        "  #\n",
        "  tmp1 = count_max_return_ge_1_prediction_0/len(data_test_tmp)*100\n",
        "  tmp2 = count_max_return_lt_1_prediction_1/len(data_test_tmp)*100\n",
        "\n",
        "  #\n",
        "  print(f\"[max_return_60min/{indicator_tmp[len(indicator_tmp) - 1]}/{percent_point}이상/1/{window_size[idx_window]}] : {tmp1}\")\n",
        "  print(f\"[max_return_60min/{indicator_tmp[len(indicator_tmp) - 1]}/{percent_point}미만/0/{window_size[idx_window]}] : {tmp2}\")\n",
        "  print(f\"[확률/{indicator_tmp[len(indicator_tmp) - 1]}/{percent_point}/{window_size[idx_window]}] : {tmp1 + tmp2}\")\n",
        "\n",
        "  print(f\"---------END({indicator_tmp[len(indicator_tmp) - 1]})------------\")"
      ],
      "metadata": {
        "id": "8WqvtMcokplC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}