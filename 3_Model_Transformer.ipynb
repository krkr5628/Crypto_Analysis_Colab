{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWlH/3XsPIS8e32t9JU2iZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAoHe8iJpIal"
      },
      "outputs": [],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "id": "8SmRHOsmvVSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tdqm"
      ],
      "metadata": {
        "id": "S55dkQZ9vWNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# TensorFlow 및 TPU 설정\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "L6izwRfIvXOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9SW8uIPVvuzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**메인 데이터 호출**\n",
        "######################"
      ],
      "metadata": {
        "id": "br8_MkBtvzhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path1 = '/content/drive/MyDrive/Data/SOL60_INDICATOR3'\n",
        "data = pd.read_csv(file_path1)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iEbnz0QJvxPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "fwAJGNlVxp-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data = data.drop(columns=['Unnamed: 0.1'])"
      ],
      "metadata": {
        "id": "3_Jq4kccx2cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data = data.drop(columns=['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "TptyKo7Hx2uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**메인 데이터 처리**\n",
        "######################"
      ],
      "metadata": {
        "id": "0BB3OoQqy8dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수\n",
        "def preprocess_data(data):\n",
        "    # 목표 변수 생성\n",
        "    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n",
        "\n",
        "    # 특성과 목표 변수 분리\n",
        "    X = data.drop(columns=['max_return_60min', 'min_return_60min', 'target'])\n",
        "    y = data['target']\n",
        "\n",
        "    # 무한대 값을 NaN으로 대체\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # NaN 값을 평균으로 대체\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# 시계열 데이터 형태로 변환 함수\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        label = target[i + sequence_length - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 데이터 전처리\n",
        "X_scaled, y = preprocess_data(data)\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "# 데이터 길이 체크\n",
        "if len(X_scaled) < sequence_length:\n",
        "    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n",
        "\n",
        "# 시퀀스 데이터 생성\n",
        "y_array = y.values  # pandas Series를 numpy array로 변환\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "# 생성된 시퀀스 데이터의 형태 확인\n",
        "print(f\"X_seq shape: {X_seq.shape}\")\n",
        "print(f\"y_seq shape: {y_seq.shape}\")"
      ],
      "metadata": {
        "id": "QE52po5Kx8tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수 V2\n",
        "def preprocess_data(data):\n",
        "    # 목표 변수 생성\n",
        "    data['target'] = (data['max_return_60min'] >= 1.1).astype(int)\n",
        "\n",
        "    # 피처 열만 선택\n",
        "    feature_columns = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10',\n",
        "                       'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30',\n",
        "                       'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200',\n",
        "                       'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20',\n",
        "                       'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200',\n",
        "                       'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20',\n",
        "                       'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40',\n",
        "                       'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20',\n",
        "                       'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close',\n",
        "                       'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50',\n",
        "                       'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n",
        "    # 특성과 목표 변수 분리\n",
        "    X = data[feature_columns]\n",
        "    y = data['target']\n",
        "\n",
        "    # 무한대 값을 NaN으로 대체\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # NaN 값을 평균으로 대체\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# 시계열 데이터 형태로 변환 함수\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        label = target[i + sequence_length - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(label)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 데이터 전처리\n",
        "X_scaled, y = preprocess_data(data)\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "# 데이터 길이 체크\n",
        "if len(X_scaled) < sequence_length:\n",
        "    raise ValueError(f\"데이터 길이({len(X_scaled)})가 시퀀스 길이({sequence_length})보다 짧습니다.\")\n",
        "\n",
        "# 시퀀스 데이터 생성\n",
        "y_array = y.values  # pandas Series를 numpy array로 변환\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_array, sequence_length)\n",
        "\n",
        "# 생성된 시퀀스 데이터의 형태 확인\n",
        "print(f\"X_seq shape: {X_seq.shape}\")\n",
        "print(f\"y_seq shape: {y_seq.shape}\")"
      ],
      "metadata": {
        "id": "WRUM44g0yAGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**테스트 데이터 호출**\n",
        "######################"
      ],
      "metadata": {
        "id": "SpAFt3pQx7JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Kaggle\n",
        "file_path2 = '/content/drive/MyDrive/Data/SOL60_INDICATOR3'\n",
        "data_test_tmp = pd.read_csv(file_path2)\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it is loaded correctly\n",
        "data_test_tmp.head()"
      ],
      "metadata": {
        "id": "lyzF4122x6ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_tmp.columns"
      ],
      "metadata": {
        "id": "bLDi8rYUy_RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data_test_tmp = data_test_tmp.drop(columns=['Unnamed: 0.1'])"
      ],
      "metadata": {
        "id": "2WnhAv0pzAzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data_test_tmp = data_test_tmp.drop(columns=['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "Wdnl32CozCTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원형\n",
        "data_test = data_test_tmp"
      ],
      "metadata": {
        "id": "rHWOsIPIzQAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**테스트 데이터 분리**\n",
        "######################"
      ],
      "metadata": {
        "id": "yM1ILhQlzXzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#12등분 => 1개월\n",
        "def extract_last_twelfth(df):\n",
        "    num_rows = len(df)\n",
        "    twelfth_size = num_rows // 12\n",
        "    start_index = 11 * twelfth_size\n",
        "    end_index = num_rows\n",
        "    last_twelfth = df.iloc[start_index:end_index]\n",
        "    return last_twelfth\n",
        "\n",
        "# 데이터프레임 12등분하여 마지막 등분 추출\n",
        "data_test = extract_last_twelfth(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "PLXUJGsZzZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6등분 => 2개월\n",
        "def extract_last_sixth(df):\n",
        "    num_rows = len(df)\n",
        "    sixth_size = num_rows // 6\n",
        "    start_index = 5 * sixth_size\n",
        "    end_index = num_rows\n",
        "    last_sixth = df.iloc[start_index:end_index]\n",
        "    return last_sixth\n",
        "\n",
        "# 데이터프레임 6등분하여 마지막 등분 추출\n",
        "data_test = extract_last_sixth(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "UTF7FHSlzdg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4등분 => 3개월\n",
        "def extract_last_fourth(df):\n",
        "    num_rows = len(df)\n",
        "    fourth_size = num_rows // 4\n",
        "    start_index = 3 * fourth_size\n",
        "    end_index = num_rows\n",
        "    last_fourth = df.iloc[start_index:end_index]\n",
        "    return last_fourth\n",
        "\n",
        "# 데이터프레임 4등분하여 마지막 등분 추출\n",
        "data_test = extract_last_fourth(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "sH0PN_EczfwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3등분 => 4개월\n",
        "def extract_last_third(df):\n",
        "    num_rows = len(df)\n",
        "    third_size = num_rows // 3\n",
        "    start_index = 2 * third_size\n",
        "    end_index = num_rows\n",
        "    last_third = df.iloc[start_index:end_index]\n",
        "    return last_third\n",
        "\n",
        "# 데이터프레임 3등분하여 마지막 등분 추출\n",
        "data_test = extract_last_third(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "Zqvaoq_xzhPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12등분 => 5개월\n",
        "def extract_last_five_parts(df):\n",
        "    num_rows = len(df)\n",
        "    part_size = num_rows // 12  # 각 등분의 크기 계산\n",
        "    start_index = part_size * 7  # 맨 뒤 7등분의 시작 인덱스\n",
        "    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n",
        "    return last_five_parts\n",
        "\n",
        "# 데이터프레임 2등분하여 마지막 등분 추출\n",
        "data_test = extract_last_five_parts(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "xciMLSZpzjCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2등분 => 6개월\n",
        "def extract_last_half(df):\n",
        "    num_rows = len(df)\n",
        "    half_size = num_rows // 2\n",
        "    start_index = half_size\n",
        "    end_index = num_rows\n",
        "    last_half = df.iloc[start_index:end_index]\n",
        "    return last_half\n",
        "\n",
        "# 데이터프레임 2등분하여 마지막 등분 추출\n",
        "data_test = extract_last_half(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "FExq_mkkzlC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12등분 => 7개월\n",
        "def extract_last_five_parts(df):\n",
        "    num_rows = len(df)\n",
        "    part_size = num_rows // 12  # 각 등분의 크기 계산\n",
        "    start_index = part_size * 5  # 맨 뒤 5등분의 시작 인덱스\n",
        "    last_five_parts = df.iloc[start_index:num_rows]  # 시작 인덱스부터 끝까지 추출\n",
        "    return last_five_parts\n",
        "\n",
        "# 데이터프레임 2등분하여 마지막 등분 추출\n",
        "data_test = extract_last_five_parts(data_test_tmp)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "LNAZtTDJznCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**테스트 데이터 처리**\n",
        "######################"
      ],
      "metadata": {
        "id": "GT0YI_90zD1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open_time 열을 datetime 형식으로 변환\n",
        "if not np.issubdtype(data_test['open_time'].dtype, np.datetime64):\n",
        "    data_test['open_time'] = pd.to_datetime(data_test['open_time'])\n",
        "\n",
        "# time 열을 분 단위로 변환\n",
        "data_test['time'] = data_test['open_time'].dt.hour * 60 + data_test['open_time'].dt.minute"
      ],
      "metadata": {
        "id": "0YOA0GgOzUA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외\n",
        "data_test_predict = data_test.drop(columns=['open_time', 'max_return_60min', 'min_return_60min'])"
      ],
      "metadata": {
        "id": "Kk5tTHS4z0Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하지 않을 열 제외 v2\n",
        "# 피처 목록\n",
        "features_to_keep = ['ichimoku_conversion_9', 'ichimoku_conversion_200', 'supertrend_upper_14_2_10',\n",
        "                    'supertrend_upper_10_3_20', 'bollinger_hband_200', 'volume_ma_100', 'ROC_30',\n",
        "                    'open', 'high', 'supertrend_lower_10_3_20', 'obv', 'atr_50', 'volume_ma_200',\n",
        "                    'Accumulation_Distribution_Line', 'bollinger_lband_20', 'lowerband', 'volume_ma_20',\n",
        "                    'supertrend_lower_7_3_14', 'atr_14', 'disparity_index_100', 'price_ma_200',\n",
        "                    'bollinger_lband_50', 'ichimoku_conversion_52', 'upperband', 'atr_20', 'price_ma_20',\n",
        "                    'disparity_index_20', 'time', 'vwap', 'bollinger_lband_200', 'atr_10', 'MFI_40',\n",
        "                    'volume_ma_10', 'supertrend_in_uptrend_7_3_14', 'Momentum_30', 'Momentum_20',\n",
        "                    'supertrend_upper_20_4_50', 'bollinger_hband_100', 'MFI_50', 'CMO_50', 'close',\n",
        "                    'Momentum_50', 'stoch_%k_21_5', 'supertrend_upper_7_3_14', 'bollinger_hband_50',\n",
        "                    'Parabolic_SAR_0.02', 'bollinger_lband_100', 'stoch_%k_9_3', 'Williams_%R_30', 'CMO_40']\n",
        "\n",
        "# 피처들만 남기기\n",
        "data_test_predict = data_test[features_to_keep]"
      ],
      "metadata": {
        "id": "LFK-0uRmz7By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 무한대 값을 NaN으로 대체\n",
        "data_test_predict.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# NaN 값을 평균으로 대체\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_test_predict_imputed = imputer.fit_transform(data_test_predict)  # 같은 imputer 사용\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = MinMaxScaler()\n",
        "data_test_predict_scaled = scaler.fit_transform(data_test_predict_imputed)  # 같은 scaler 사용\n",
        "\n",
        "# 예측 데이터를 시퀀스 형태로 변환 (LSTM용)\n",
        "def create_sequences_for_prediction(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        seq = data[i:i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "# 시퀀스 길이 설정\n",
        "sequence_length = 60\n",
        "\n",
        "# 예측용 시퀀스 데이터 생성\n",
        "X_test_seq = create_sequences_for_prediction(data_test_predict_scaled, sequence_length)"
      ],
      "metadata": {
        "id": "sPo9dkDKz9uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**Transformer 신규 학습**\n",
        "######################"
      ],
      "metadata": {
        "id": "rucaq2BH0Btz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "MYq10xJ70EM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터를 텐서로 변환\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "6ZnfbKud0Uw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerEncoder 모델 정의\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, input_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n",
        "        return output\n",
        "\n",
        "# 모델 설정\n",
        "input_dim = X_train.shape[2]\n",
        "nhead = 2\n",
        "num_layers = 2\n",
        "dim_feedforward = 64\n",
        "output_dim = 1\n",
        "\n",
        "model = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n",
        "\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습률 감소 스케줄러 설정\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# 조기 종료 설정\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# 학습 및 검증 손실을 저장할 리스트\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# 모델 학습\n",
        "num_epochs = 20  # 최대 에포크 수\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # 검증 단계\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Duration: {epoch_duration:.2f}s')\n",
        "\n",
        "    # 최적의 모델 저장 및 조기 종료 조건 체크\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n",
        "        break\n",
        "\n",
        "    # 학습률 감소\n",
        "    scheduler.step()\n",
        "\n",
        "# 학습 및 검증 손실 시각화\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        output = model(X_batch)\n",
        "        y_true.extend(y_batch.tolist())\n",
        "        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n",
        "\n",
        "# 이진 분류 결과를 위한 평가 지표 계산\n",
        "y_pred = np.array(y_pred) > 0.5\n",
        "y_true = y_test_tensor.numpy()\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "82oinTLd0W-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 경로\n",
        "model_path = '/content/drive/MyDrive/Data/Model/SOL60_SMALLL_INDICATOR3_Transformer.pth'\n",
        "\n",
        "# 모델 상태 저장\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "E0n7ai3Z0a6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**Transformer추가 학습**\n",
        "######################"
      ],
      "metadata": {
        "id": "EWHkJZUn0FXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "AcvuQunJ0voc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerEncoder 모델 정의\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, input_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n",
        "        return output\n",
        "\n",
        "# 입력 차원 확인 및 설정\n",
        "input_dim = X_train.shape[2]\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "# 모델 설정 (로드할 때 필요)\n",
        "nhead = 2\n",
        "num_layers = 2\n",
        "dim_feedforward = 64\n",
        "output_dim = 1\n",
        "\n",
        "model = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n",
        "\n",
        "# 모델 로드\n",
        "model_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/6/SOL60_SMALLL_INDICATOR3_Transformer_v3.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.train()  # 추가 학습을 위해 학습 모드로 전환\n",
        "print(f\"Model loaded from {model_path}\")"
      ],
      "metadata": {
        "id": "MMlIR3aj0MNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터를 텐서로 변환\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "SE7cUGMI0x88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 조기 종료 설정\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# 학습 및 검증 손실을 저장할 리스트\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# 추가 학습\n",
        "num_epochs = 100  # 최대 에포크 수\n",
        "for epoch in range(num_epochs):\n",
        "    # 학습 단계\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # 검증 단계\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # 조기 종료 조건 체크\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"조기 종료 조건 충족. 학습을 중지합니다.\")\n",
        "        break\n",
        "\n",
        "# 학습 및 검증 손실 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 모델 평가\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        output = model(X_batch)\n",
        "        y_true.extend(y_batch.tolist())\n",
        "        y_pred.extend(torch.sigmoid(output).squeeze().tolist())\n",
        "\n",
        "# 이진 분류 결과를 위한 평가 지표 계산\n",
        "y_pred = np.array(y_pred) > 0.5\n",
        "y_true = y_test_tensor.numpy()\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "3c_r__Vc07pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 경로\n",
        "model_path = '/content/drive/MyDrive/Data/Model/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\n",
        "\n",
        "# 모델 상태 저장\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "wfLszR_D1BK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**Transformer 모델 로드**\n",
        "######################"
      ],
      "metadata": {
        "id": "LPP5hKZp0InU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "NZhyeSe_1Lcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의 (로드할 때 필요)\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_layers, dim_feedforward, output_dim):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, input_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.fc(output[:, -1, :])  # Use the output from the last time step\n",
        "        return output\n",
        "\n",
        "# 입력 차원 확인 및 설정\n",
        "input_dim = X_test_seq.shape[2]\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "# 모델 설정 (로드할 때 필요)\n",
        "nhead = 2\n",
        "num_layers = 2\n",
        "dim_feedforward = 64\n",
        "output_dim = 1\n",
        "\n",
        "model = TransformerEncoderModel(input_dim, nhead, num_layers, dim_feedforward, output_dim)\n",
        "\n",
        "# 모델 로드\n",
        "model_path = '/kaggle/input/lstm_indiactor3/pytorch/tcn_transformer/5/SOL60_SMALLL_INDICATOR3_Transformer_v2.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "print(f\"Model loaded from {model_path}\")"
      ],
      "metadata": {
        "id": "2S1-_qZK0MyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "bIoE5eZ96VAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**Transformer 모델 테스트**\n",
        "######################"
      ],
      "metadata": {
        "id": "8yPS8uJz1RPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2등분\n",
        "def extract_last_half(df):\n",
        "    num_rows = len(df)\n",
        "    half_size = num_rows // 2\n",
        "    start_index = half_size\n",
        "    end_index = num_rows\n",
        "    last_half = df.iloc[start_index:end_index]\n",
        "    return last_half\n",
        "\n",
        "# 데이터프레임 2등분하여 마지막 등분 추출\n",
        "data_test = extract_last_half(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "Ug53jSRa19N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3등분\n",
        "def extract_last_third(df):\n",
        "    num_rows = len(df)\n",
        "    third_size = num_rows // 3\n",
        "    start_index = 2 * third_size\n",
        "    end_index = num_rows\n",
        "    last_third = df.iloc[start_index:end_index]\n",
        "    return last_third\n",
        "\n",
        "# 데이터프레임 3등분하여 마지막 등분 추출\n",
        "data_test = extract_last_third(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "dNjhRC3o1_AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4등분하여 마지막 등분을 추출하는 함수\n",
        "def extract_last_quarter(df):\n",
        "    num_rows = len(df)\n",
        "    quarter_size = num_rows // 4\n",
        "    start_index = 3 * quarter_size\n",
        "    end_index = num_rows\n",
        "    last_quarter = df.iloc[start_index:end_index]\n",
        "    return last_quarter\n",
        "\n",
        "# 데이터프레임 4등분하여 마지막 등분 추출\n",
        "data_test = extract_last_quarter(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "2yoEcTUz2ApJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5등분하여 마지막 등분을 추출하는 함수\n",
        "def extract_last_sixth(df):\n",
        "    num_rows = len(df)\n",
        "    fifth_size = num_rows // 5\n",
        "    start_index = 4 * fifth_size\n",
        "    end_index = num_rows\n",
        "    last_sixth = df.iloc[start_index:end_index]\n",
        "    return last_sixth\n",
        "\n",
        "# 데이터프레임 5등분하여 마지막 등분 추출\n",
        "data_test = extract_last_sixth(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "RB-zg2w72CW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6등분하여 마지막 등분을 추출하는 함수\n",
        "def extract_last_sixth(df):\n",
        "    num_rows = len(df)\n",
        "    sixth_size = num_rows // 6\n",
        "    start_index = 5 * sixth_size\n",
        "    end_index = num_rows\n",
        "    last_sixth = df.iloc[start_index:end_index]\n",
        "    return last_sixth\n",
        "\n",
        "# 데이터프레임 6등분하여 마지막 등분 추출\n",
        "data_test = extract_last_sixth(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "gTLoSj9F2EmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12등분 => 1개월\n",
        "def extract_last_twelfth(df):\n",
        "    num_rows = len(df)\n",
        "    twelfth_size = num_rows // 12\n",
        "    start_index = 11 * twelfth_size\n",
        "    end_index = num_rows\n",
        "    last_twelfth = df.iloc[start_index:end_index]\n",
        "    return last_twelfth\n",
        "\n",
        "# 데이터프레임 12등분하여 마지막 등분 추출\n",
        "data_test = extract_last_twelfth(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "2ESfcabv2JX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7등분하여 마지막 등분을 추출하는 함수\n",
        "def extract_last_sixth(df):\n",
        "    num_rows = len(df)\n",
        "    sixth_size = num_rows // 7\n",
        "    start_index = 6 * sixth_size\n",
        "    end_index = num_rows\n",
        "    last_sixth = df.iloc[start_index:end_index]\n",
        "    return last_sixth\n",
        "\n",
        "# 데이터프레임 6등분하여 마지막 등분 추출\n",
        "data_test = extract_last_sixth(data_test)\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "PwT4wkkm2IHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence_length를 사용하여 X_test_seq의 shape를 맞춤\n",
        "sequence_length = X_test_seq.shape[1]\n",
        "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).view(-1, sequence_length, input_dim)\n",
        "\n",
        "# 예측 수행\n",
        "with torch.no_grad():\n",
        "    predictions = torch.sigmoid(model(X_test_tensor)).squeeze().numpy()\n",
        "\n",
        "# 예측 결과를 이진 분류로 변환 (0 또는 1)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# 예측 결과를 데이터프레임에 추가\n",
        "data_test['prediction_Transformer'] = np.nan  # 예측 결과를 담을 열을 초기화\n",
        "data_test.iloc[sequence_length - 1:sequence_length - 1 + len(predictions), data_test.columns.get_loc('prediction_Transformer')] = predictions\n",
        "\n",
        "# 결과 확인\n",
        "print(data_test[['open_time', 'prediction_Transformer']].head())"
      ],
      "metadata": {
        "id": "DNq7-RqC1TA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_non_nan = data_test.dropna(subset=['prediction_Transformer'])\n",
        "\n",
        "# max_return_60min 기준으로 내림차순 정렬\n",
        "data_test_sorted = data_test_non_nan.sort_values(by='max_return_60min', ascending=False)\n",
        "\n",
        "# 결과 출력\n",
        "data_test_sorted"
      ],
      "metadata": {
        "id": "SFqWRejV1ZP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'max_return_60min' 값이 1 이상이고 'prediction' 값이 0인 데이터의 개수\n",
        "count_max_return_ge_1_prediction_0 = len(data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 1)])\n",
        "\n",
        "# 'max_return_60min' 값이 1 미만이고 'prediction' 값이 1인 데이터의 개수\n",
        "count_max_return_lt_1_prediction_1 = len(data_test[(data_test['max_return_60min'] < 1.1) & (data_test['prediction_Transformer'] == 0)])\n",
        "\n",
        "print(f\"max_return_60min이 1 이상인데 prediction이 0인 데이터의 비율: {count_max_return_ge_1_prediction_0/len(data_test)*100}\")\n",
        "print(f\"max_return_60min이 1 미만인데 prediction이 1인 데이터의 비율: {count_max_return_lt_1_prediction_1/len(data_test)*100}\")"
      ],
      "metadata": {
        "id": "gdN0E7gV1v8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################\n",
        "\n",
        "**Transformer 모델 시간대 테스트**\n",
        "######################"
      ],
      "metadata": {
        "id": "79bswWQ82RiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 조건을 만족하는 데이터 필터링\n",
        "filtered_data = data_test[(data_test['max_return_60min'] >= 1.1) & (data_test['prediction_Transformer'] == 0)]\n",
        "\n",
        "# min_return_60min 값의 분포를 히스토그램으로 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(filtered_data['min_return_60min'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of min_return_60min for max_return_60min >= 1.1 and prediction_Transformer == 0')\n",
        "plt.xlabel('min_return_60min')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NkhCCFzi2UwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 정의: 각 행에 대해 60행 이내의 high와 low 값을 비교하여 시각을 찾는 함수\n",
        "def find_high_low_times(df, window=60):\n",
        "    max_high_times = []\n",
        "    min_low_times = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        # 현재 행부터 60행 이내의 데이터를 선택\n",
        "        subset = df.iloc[i:i+window]\n",
        "\n",
        "        # 최대 high와 최소 low 값을 찾음\n",
        "        max_high_idx = subset['high'].idxmax()\n",
        "        min_low_idx = subset['low'].idxmin()\n",
        "\n",
        "        # 해당 인덱스의 시간을 저장\n",
        "        max_high_time = df.at[max_high_idx, 'open_time']\n",
        "        min_low_time = df.at[min_low_idx, 'open_time']\n",
        "\n",
        "        max_high_times.append(max_high_time)\n",
        "        min_low_times.append(min_low_time)\n",
        "\n",
        "    df['max_high_time'] = max_high_times\n",
        "    df['min_low_time'] = min_low_times\n",
        "\n",
        "    return df\n",
        "\n",
        "# 함수 호출\n",
        "data_test = find_high_low_times(data_test)\n",
        "data_test"
      ],
      "metadata": {
        "id": "nXbykKIi2Ybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 조건에 맞는 새로운 열 추가\n",
        "def add_condition_column(df):\n",
        "    # 'max_high_time'과 'min_low_time'이 datetime 형식인지 확인하고 변환\n",
        "    df['max_high_time'] = pd.to_datetime(df['max_high_time'])\n",
        "    df['min_low_time'] = pd.to_datetime(df['min_low_time'])\n",
        "\n",
        "    # 조건에 맞는 행 필터링 (max_return_60min >= 1.1 및 prediction_Transformer == 1)\n",
        "    filtered_df = df[(df['max_return_60min'] >= 1.1) & (df['prediction_Transformer'] == 1)].copy()\n",
        "\n",
        "    # 새로운 열 추가 및 초기화\n",
        "    filtered_df['condition'] = 0\n",
        "\n",
        "    # 조건을 만족하는 경우 condition 값을 설정\n",
        "    condition_indices = filtered_df.index[filtered_df['max_high_time'] < filtered_df['min_low_time']]\n",
        "\n",
        "    filtered_df.loc[condition_indices, 'condition'] = 1\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# 데이터 타입 변환\n",
        "data_test['max_high_time'] = pd.to_datetime(data_test['max_high_time'])\n",
        "data_test['min_low_time'] = pd.to_datetime(data_test['min_low_time'])\n",
        "\n",
        "# 조건 열 추가\n",
        "filtered_data_test = add_condition_column(data_test)\n",
        "\n",
        "# 결과 확인\n",
        "filtered_data_test"
      ],
      "metadata": {
        "id": "eGy-zV1R2dKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "condition_1_data = filtered_data_test[filtered_data_test['condition'] == 0]\n",
        "condition_1_data"
      ],
      "metadata": {
        "id": "_k0ScB0S2nhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_high_time과 min_low_time의 차이를 분으로 계산하여 새로운 열 추가\n",
        "condition_1_data['time_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['min_low_time']).dt.total_seconds() / 60\n",
        "condition_1_data"
      ],
      "metadata": {
        "id": "yzS7GTIH2oAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#min_low_time과 open_time의 차이를 분으로 계산하여 새로운 열 추가\n",
        "condition_1_data['min_low_open_difference_minutes'] = (condition_1_data['max_high_time'] - condition_1_data['open_time']).dt.total_seconds() / 60\n",
        "condition_1_data"
      ],
      "metadata": {
        "id": "Uh9Z7knB2pDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\n",
        "time_difference_max = condition_1_data['min_low_open_difference_minutes'].max()\n",
        "time_difference_min = condition_1_data['min_low_open_difference_minutes'].min()\n",
        "time_difference_mode = condition_1_data['min_low_open_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\n",
        "time_difference_mean = condition_1_data['min_low_open_difference_minutes'].mean()\n",
        "\n",
        "# 결과 출력\n",
        "time_difference_stats = {\n",
        "    'max': time_difference_max,\n",
        "    'min': time_difference_min,\n",
        "    'mode': time_difference_mode, #최빈\n",
        "    'mean': time_difference_mean # 평균\n",
        "}\n",
        "\n",
        "time_difference_stats"
      ],
      "metadata": {
        "id": "Y4rn4rPQ2q6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min_return_60min이 -1 이하인 행들 필터링\n",
        "negative_return_data = condition_1_data[condition_1_data['min_return_60min'] <= -1]\n",
        "\n",
        "# time_difference_minutes의 최대, 최소, 최빈, 평균값 계산\n",
        "time_difference_max = negative_return_data['time_difference_minutes'].max()\n",
        "time_difference_min = negative_return_data['time_difference_minutes'].min()\n",
        "time_difference_mode = negative_return_data['time_difference_minutes'].mode()[0]  # 최빈값이 여러 개일 경우 첫 번째 값 선택\n",
        "time_difference_mean = negative_return_data['time_difference_minutes'].mean()\n",
        "\n",
        "# 결과 출력\n",
        "time_difference_stats = {\n",
        "    'max': time_difference_max,\n",
        "    'min': time_difference_min,\n",
        "    'mode': time_difference_mode,  # 최빈값\n",
        "    'mean': time_difference_mean   # 평균값\n",
        "}\n",
        "\n",
        "time_difference_stats"
      ],
      "metadata": {
        "id": "_nBiqAl_2uzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}